# a quick guide to install and configure hadoop on ubuntu linux
# by justasabc 2013.10.19
#---------------------------------------------------------------------
# 1) Java install
# http://atuljha.com/blog/2012/05/21/notes-installing-singlemulti-node-hadoop-cluster-on-ubuntu-12-04/
# https://github.com/flexiondotorg/oab-java6
# 2) single node
# http://hadoop.apache.org/docs/stable/single_node_setup.html
# http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/
# 3) multi node
# http://hadoop.apache.org/docs/stable/cluster_setup.html
# http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/
#---------------------------------------------------------------------

# ********************************************************************
# The JDK is a superset of the JRE, and contains everything that is in the JRE
# jdk contains jre
# jdk contains some useful tools such as: jps

# 4) use bin from oracle (RECOMMENDED)
# download page
# http://www.oracle.com/technetwork/java/javase/downloads/jdk6u35-downloads-1836443.html
cd /usr/lib/jvm/
wget http://.../jdk-6u35-linux-x64.bin
./jdk-6u35-linux-x64.bin
ln -s /usr/lib/jvm/jdk1.6.0_35/  /usr/lib/jvm/javahome

# set JAVA_HOME path for all users
# http://www.cyberciti.biz/faq/linux-unix-set-java_home-path-variable/
vim /etc/profile
export JAVA_HOME=/usr/lib/jvm/javahome
export PATH=$PATH:$JAVA_HOME/bin
source /etc/profile
echo $JAVA_HOME
echo $PATH

# test java
java
java -version

# ********************************************************************
# Adding a dedicated Hadoop system user
sudo addgroup hadoop
# cat /etc/group | grep hadoop
sudo adduser --ingroup hadoop hduser
# Unix password is 1234

# cat /etc/passwd | grep hadoop
# hduser:x:1001:1001:hadoop,419,,:/home/hduser:/bin/bash

# add permission for hduser
su root
chmod 640 /etc/sudoers
vim /etc/sudoers
hduser  ALL=(ALL:ALL) ALL

# ********************************************************************
# Configuring SSH
# For our single-node setup of Hadoop, we therefore need to configure SSH access to localhost for the hduser user we created in the previous section.
su - hduser
ssh-keygen -t rsa -P ""
cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys
ssh localhost
# yes
exit

# Disabling IPv6
vim /etc/sysctl.conf
# disable ipv6
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
# You have to reboot your machine in order to make the changes take effect.
# You can check whether IPv6 is enabled on your machine with the following command:
cat /proc/sys/net/ipv6/conf/all/disable_ipv6
# 0 enabled, 1 disable

# Alternative (later)
vim hadoop/conf/hadoop-env.sh
export HADOOP_OPTS=-Djava.net.preferIPv4Stack=true

# hadoop installation
# download hadoop
# http://hadoop.apache.org/releases.html
wget http://apache.dataguru.cn/hadoop/core/hadoop-1.2.1/hadoop-1.2.1.tar.gz
tar xzf hadoop-1.2.1.tar.gz
mv hadoop-1.2.1.tar.gz hadoop
sudo chown -R hduser:hadoop hadoop
# in fact ,no need to do so

# configure JAVA_HOME
vim hadoop/conf/hadoop-env.sh
export JAVA_HOME=/usr/lib/jvm/javahome

# add java/bin hadoop/bin to $PATH
# No need !!!
#vim /home/hduser/.bashrc
#export JAVA_HOME=/usr/lib/jvm/javahome
# Add java bin/ directory to PATH
#export PATH=$PATH:$JAVA_HOME/bin
# Add Hadoop bin/ directory to PATH
#export PATH=$PATH:$HADOOP_HOME/bin

# *******************************************************************
# 1) Standalone Operation (skip)
# By default, Hadoop is configured to run in a non-distributed mode, as a single Java process. This is useful for debugging.
cd haoop
bin/hadoop jar hadoop-examples-*.jar grep conf/*.xml output 'dfs[a-z.]+'
cat output/*
# success

# *******************************************************************
# 2) pseudo-Distributed Operation local mode

#conf/core-site.xml:
<configuration>
     <property>
         <name>fs.default.name</name>
         <value>hdfs://localhost:54310</value>
     </property>
</configuration>

#conf/mapred-site.xml:
<configuration>
     <property>
         <name>mapred.job.tracker</name>
         <value>localhost:54311</value>
     </property>
</configuration>

#conf/hdfs-site.xml:
<configuration>
     <property>
         <name>dfs.replication</name>
         <value>1</value>
     </property>
</configuration>


# format a new distributed-filesystem:
bin/hadoop namenode -format

#Start the hadoop daemons:
bin/start-all.sh

# check processes
ps -ef | grep java
# too long args,use jps instead
jps
# see java processes
hduser@ubuntu:~/hadoop/conf$ jps
18938 Jps
11486 JobTracker
11706 TaskTracker
11401 SecondaryNameNode
11189 DataNode
10949 NameNode

# stop the hadoop daemons
bin/stop-all.sh

# *******************************************************************
# 3) Fully-Distributed Operation
# for master
vim /etc/hostname
master
# for slave
vim /etc/hostname
slave
reboot
# master slave
vim /etc/hosts
192.168.1.200 master
192.168.1.201 slave

# test ssh
ssh master

# ssh from master to slave via password-less SSH login
# add master's pub key to slave's authorized_keys
ssh-copy-id -i $HOME/.ssh/id_rsa.pub hduser@slave
# will prompt password for hduser from slave (1234)

# This command will prompt you for the login password for user hduser on slave, then copy the public SSH key for you, creating the correct directory and fixing the permissions as necessary.

# now we can ssh from master to slave WITHOUT password
hduser@master:~$  ssh hduser@slave

ssh hduser@slave
ssh slave

# configuration
# NameNode , JobTracker--->master daemons
# SecondaryNameNode    --->master daemons
# DataNode , TaskTracker--->slave daemons
# 
# NameNode --- site-core.xml, hdfs-site.xml 
# JobTracker---mapred-site.xml
# 
# bin/start-dfs.sh 
# NameNode in hdfs layer (data storage)
# DataNode in conf/slaves
#
# SecondaryNameNode in conf/masters

# Why?
# bin/start-dfs.sh in conf/masters --->SecondaryNameNode in hdfs layer
# The secondary NameNode is started by “bin/start-dfs.sh“ on the nodes specified in “conf/masters“ file.

# conf/masters (master only)
vim conf/masters
master
# conf/masters (slave node)
localhost

# bin/start-mapred.sh
# JobTracker in mapreduce layer(data processing)
# TaskTracker in conf/slaves

# Why?
# The conf/slaves file lists the hosts, one per line, where the Hadoop slave daemons (DataNodes and TaskTrackers) will be run. 

# conf/slaves (master only)
vim conf/slaves
master
slave
# conf/slaves(slave node)
localhost

# hadoop.tmp.dir
sudo mkdir /home/hduser/tmp
sudo chown hduser:hadoop /home/hduser/tmp
sudo chmod 750 /home/hduser/tmp
# If you forget to set the required ownerships and permissions, you will see a java.io.IOException when you try to format the name node in the next section).

# config for all nodes in core-site.xml, mapred-site.xml,hdfs-site.xml
#conf/core-site.xml:
<configuration>
     <property>
         <name>hadoop.tmp.dir</name>
         <value>/home/hduser/tmp</value>
     </property>
     <property>
         <name>fs.default.name</name>
         <value>hdfs://master:54310</value>
     </property>
</configuration>

#conf/mapred-site.xml:
<configuration>
     <property>
         <name>mapred.job.tracker</name>
         <value>master:54311</value>
     </property>
</configuration>

#conf/hdfs-site.xml:
# there are 2 DataNodes
<configuration>
     <property>
         <name>dfs.replication</name>
         <value>2</value>
     </property>
</configuration>

# conclusion
# 1) *.xml ,for all nodes
# 2) edit conf/masters, conf/slaves for master node ONLY
#    keep conf/masters, conf/slaves for slave node to 'localhost'
# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
# 3) The only difference between Master and Slave node is:
# master node: conf/masters and conf/slaves are user defined
# slave node: conf/masters and conf/slaves are default 'localhost'
# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

bin/hadoop namenode -format
#storage directory /tmp/hadoop-hduser/dfs/name has been successfully formatted.
# by default, /tmp/hadoop-hduser
# but in this case , /home/hduser/tmp


#Starting the multi-node cluster
#Starting the cluster is performed in two steps.

#We begin with starting the HDFS daemons: the NameNode daemon is started on master, and DataNode daemons are started on all slaves (here: master and slave).
#Then we start the MapReduce daemons: the JobTracker is started on master, and TaskTracker daemons are started on all slaves (here: master and slave)

# in master
bin/start-dfs.sh 

jps 
# or
sudo netstat -plten | grep java
# web port
sudo netstat -plten | grep 500
# tcp socket
sudo netstat -plten | grep 543

#hduser@master:~/hadoop/bin$ jps
8012 NameNode
8439 SecondaryNameNode
8223 DataNode

#hduser@slave:~/hadoop/bin$ jps
5593 DataNode

# in master
#hduer@master:~/hadoop/bin$ ./stop-dfs.sh 
stopping namenode
slave: stopping datanode
master: stopping datanode
master: stopping secondarynamenode

#hduer@slave:~/hadoop/bin$ jps
5826 Jps

# in master
bin/start-mapred.sh 

#hduser@master:~/hadoop/bin$ jps
9012 JobTracker
9216 TaskTracker

#hduser@slave:~/hadoop/bin$ jps
6004 TaskTracker

#hduser@master:~/hadoop/bin$ ./stop-mapred.sh 
stopping jobtracker
master: stopping tasktracker
slave: stopping tasktracker

#hduser@slave:~/hadoop/bin$ jps
6228 Jps

# Startup scripts
# -----------------------------------------------------------------
# http://wiki.apache.org/hadoop/GettingStartedWithHadoop
# -----------------------------------------------------------------
#The hadoop/bin directory contains some scripts used to launch Hadoop DFS and Hadoop Map/Reduce daemons. These are:

#start-dfs.sh - Starts the Hadoop DFS daemons, the namenode and datanodes. Use this before start-mapred.sh
#stop-dfs.sh - Stops the Hadoop DFS daemons.
#start-mapred.sh - Starts the Hadoop Map/Reduce daemons, the jobtracker and tasktrackers.
#stop-mapred.sh - Stops the Hadoop Map/Reduce daemons.
#start-all.sh - Starts all Hadoop daemons, the namenode, datanodes, the jobtracker and tasktrackers. Deprecated; use start-dfs.sh then start-mapred.sh
#stop-all.sh - Stops all Hadoop daemons. Deprecated; use stop-mapred.sh then stop-dfs.sh

# WordCount Example
# -----------------------------------------------------------------
# http://wiki.apache.org/hadoop/WordCount
# http://wiki.apache.org/hadoop/ImportantConcepts
# -----------------------------------------------------------------
# stop all nodes and the format dfs
bin/stop-all.sh
bin/hadoop namenode -format
# Storage directory /home/hduser/tmp/dfs/name has been successfully formatted.

# hadoop commands
bin/hadoop -help
bin/hadoop dfs -help 
bin/hadoop dfs -help put

# copy local folder to hdfs
bin/haoop dfs -copyFromLocal /home/hduser/hadoop/test /home/hdusr/tmp

# test/1.txt

# test/2.txt

# list files in hdfs
bin/hadoop dfs -ls /home/hduser/tmp
#hduer@master:~/hadoop$ bin/hadoop dfs -ls ~/tmp
drwxr-xr-x   - hduser supergroup          0 2013-10-20 02:00 /home/hduser/tmp/test
# list files in hdfs test folder
bin/hadoop dfs -ls ~/tmp/test

# Run the MapReduce job
# available commands in hadoop-examples-*.jar
bin/hadoop jar hadoop-examples-1.2.1.jar 
# wordcount pi
bin/hadoop jar hadoop-examples-1.2.1.jar wordcount
bin/hadoop jar hadoop-examples-1.2.1.jar pi

# available commands in hadoop-test-*.jar
bin/hadoop jar hadoop-test-1.2.1.jar 
# TestDFSIO nnbench mrbench
bin/hadoop jar hadoop-test-1.2.1.jar TestDFSIO
#TestDFSIO.0.0.4
#Usage: TestDFSIO -read | -write | -clean [-nrFiles N] [-fileSize MB] [-resFile resultFileName] [-bufferSize Bytes] 


# Now, we actually run the WordCount example job.
bin/hadoop jar hadoop-examples-1.2.1.jar wordcount /home/hduser/tmp/test /home/hduser/tmp/test-output

# Check if the result is successfully stored in HDFS directory /home/hduser/tmp/test-output
bin/hadoop dfs -ls ~/tmp/test-output
# hduer@master:~/hadoop$ bin/hadoop dfs -ls ~/tmp/test-output

# Retrieve the job result from HDFS
# copy it from hdfs to local filesystem
bin/hadoop dfs -get /home/hduser/tmp/test-output /home/hduser/

# or hdfs cat
#hduser@master:~/hadoop$ bin/hadoop dfs -cat ~/tmp/test-output/part-r-00000
about	1
and	1
are	1
good	1
hello	2
how	1
kezunlin	1
my	2
perfect	1
to	1
too	2
welcome	1
world	3
you	3

# Hadoop Web Interfaces
# Hadoop comes with several web interfaces which are by default (see conf/hadoop-default.xml) available at these locations:

#Hadoop comes with several web interfaces which are by default (see conf/hadoop-default.xml) available at these locations:

http://localhost:50070/ – web UI of the NameNode daemon
http://localhost:50030/ – web UI of the JobTracker daemon
http://localhost:50060/ – web UI of the TaskTracker daemon

# dfsadmin
bin/hadoop dfsadmin
bin/hadoop dfsadmin -report

# -------------------------------------------------------------------------
# write a user defined map/reduce wordcount program
# default demo
bin/hadoop jar hadoop*examples*.jar wordcount /home/temp/input /home/temp/output
# user defined WordCount 
cd /home/hduser/hadoop
mkdir wordcount_classes
vim WordCount.java
# ...
# vim src/examples/org/apache/hadoop/examples/WordCount.java
# compile wordcount java to classes
javac -classpath hadoop-core-1.2.1.jar:lib/commons-cli-1.2.jar -d wordcount_classes/ WordCount.java 
# create a jar
jar cvf wordcount.jar -C wordcount_classes/ . 

# create local input files
mkdir -p wordcount/input
echo "Hello World Bye World" > wordcount/input/file01
echo "Hello Hadoop GoodBye Hadoop" > wordcount/input/file02
# copy local input dirs/files to hdfs (if dir not exist,create it recursively)
bin/hadoop dfs -copyFromLocal wordcount/input /home/hduser/tmp/wordcount/input
# /home/hduser/tmp/wordcount/input does not exist
bin/hadoop dfs -ls /home/hduser/tmp/wordcount/input
# file01 file02

# run map/reduce job
bin/hadoop jar wordcount.jar org.apache.hadoop.examples.WordCount /home/hduser/tmp/wordcount/input /home/hduser/tmp/wordcount/output

# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
# Notice:
# 1) run for second time
bin/hadoop jar wordcount.jar org.apache.hadoop.examples.WordCount /home/hduser/tmp/wordcount/input /home/hduser/tmp/wordcount/output
# we will get an error
# Exception in thread "main" org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory /home/hduser/tmp/wordcount/output already exists

# solution: remove output in hdfs
bin/hadoop dfs -rmr /home/hduser/tmp/wordcount/output

# 2) use org.apache.hadoop.examples.WordCount instead of wordcount or WordCount
# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

# list output in hdfs
bin/hadoop dfs -ls /home/hduser/tmp/wordcount/output
#Found 3 items
#-rw-r--r--   2 hduser supergroup          0 2013-10-21 23:54 /home/hduser/tmp/wordcount/output/_SUCCESS
#drwxr-xr-x   - hduser supergroup          0 2013-10-21 23:53 /home/hduser/tmp/wordcount/output/_logs
#-rw-r--r--   2 hduser supergroup         41 2013-10-21 23:53 /home/hduser/tmp/wordcount/output/part-r-00000

# cat results
bin/hadoop dfs -cat /home/hduser/tmp/wordcount/output/part-r-00000
Bye	1
GoodBye	1
Hadoop	2
Hello	2
World	2

