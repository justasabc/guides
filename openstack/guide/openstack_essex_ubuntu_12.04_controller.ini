# A quick guide to install openstack essex on ubuntu 12.04 server adm64 on physical machine
#----------------------------------------------------------------------
# http://www.hastexo.com/resources/docs/installing-openstack-essex-20121-ubuntu-1204-precise-pangolin
# http://www.hastexo.com/resources/docs/installing-openstack-grizzly-20131-ubuntu-1204-precise-pangolin
# http://www.chenshake.com/ubuntu-12-04-openstack-essex-installation-single-node/#i-16
# http://www.chenshake.com/ubuntu12-04-2-installed-openstack-grizzly-bridge-mode/
# http://www.mirantis.com/blog/openstack-networking-flatmanager-and-flatdhcpmanager/
# http://blog.csdn.net/spch1888/article/details/7862950
#----------------------------------------------------------------------
# OpenStack Essex 2012.1.3

# Plan
#================================================
controller + compute1 + compute2
controller: 192.168.1.188
compute1: 192.168.1.189
compute2: 192.168.1.190
# check hostname in /etc/network/interfaces /etc/hostname /etc/hosts
#================================================

#Prerequisites: Ubuntu 12.04 with all package updates installed. I'm using the stock OpenStack packages as delivered by Ubuntu. This howto assumes that all relevant OpenStack services are installed on the same machine; in order to add more computing nodes, install the OpenStack nova components on these nodes and adapt /etc/nova/nova.conf accordingly. The machine I created this setup on has two network interfaces, eth0 and eth1. Last but not least, I'm assuming that you are logged in as root.

#====================================================
# Step 1: Prepare your System
#====================================================
apt-get -y update
apt-get -y upgrade

# permanently save and restore iptables rules
cat /etc/network/if-pre-up.d/iptables_restore
#**************************************************
#**************************************************
#!/bin/sh
iptables-restore < /etc/iptables.rules
exit 0
#**************************************************
#**************************************************

cat /etc/network/if-post-down.d/iptables_save
#**************************************************
#**************************************************
#!/bin/sh
iptables-save -c > /etc/iptables.rules
if [ -f /etc/iptables.rules ]; then
	iptables-restore < /etc/iptables.rules
fi
exit 0
#**************************************************
#**************************************************
chmod +x /etc/network/if-pre-up.d/iptables_restore
chmod +x /etc/network/if-post-down.d/iptables_save


#By default, the IP forwarding is disabled on most of Linux distributions. The "floating IP" feature requires the IP forwarding enabled in order to work, you can check if the forwarding is enabled by running the following command:
#Enabling IP forwarding
echo 1 > /proc/sys/net/ipv4/ip_forward
cat /proc/sys/net/ipv4/ip_forward
#1

#We need to make sure that our network is working as expected. As pointed out earlier, the machine we're doing this on has two network interfaces, eth0 and eth1. eth0 is the machine's link to the outside world, eth1 is the interface we'll be using for our virtual machines. We'll also make nova bridge clients via eth0 into the internet. To achieve this kind of setup, first create the according network configuration in /etc/network/interfaces (assuming that you are not using NetworkManager). An example could look like this:
vim /etc/network/interfaces
#**************************************************
#**************************************************
auto lo
iface lo inet loopback

auto eth0
iface eth0 inet static
        address 192.168.1.188
        netmask 255.255.255.0
        network 192.168.1.0
        broadcast 192.168.1.255
        gateway 192.168.1.1
	dns-nameservers 162.105.129.26 162.105.129.27
	dns-search example.com

auto eth1
iface eth1 inet manual
	up ifconfig eth1 up
#**************************************************
#**************************************************
/etc/init.d/networking restart

mii-tool 
#eth0: negotiated 1000baseT-FD flow-control, link ok
#eth1: negotiated 1000baseT-FD flow-control, link ok

# ===========================================================
# 3 methods to set eth1
auto eth1
iface eth1 inet manual
	up ifconfig eth1 up

# or
auto eth1
iface eth1 inet static
	address 192.168.2.1
	network 192.168.2.0
	netmask 255.255.255.0
	broadcast 192.168.2.255

# or
auto eth1
iface eth1 inet manual
	up ifconfig $IFACE 0.0.0.0 up
	up ifconfig $IFACE promisc
# ===========================================================

apt-get -y install bridge-utils
# install ntp and configure
apt-get -y install ntp
vim /etc/ntp.conf
server ntp.ubuntu.com iburst
server 127.127.1.0
fudge 127.127.1.0 stratum 10

service ntp restart
 
# install tgt target (we'll need it for nova-volume) 
apt-get -y install tgt
service tgt restart

# Given that we'll be running nova-compute on this machine as well, we'll also need the openiscsi-client.
# install iscsi client
apt-get -y install open-iscsi open-iscsi-utils

# We'll also need RabbitMQ, an AMQP-implementation, as that is what all OpenStack components use to communicate with eath other, and memcached.
apt-get -y install rabbitmq-server memcached python-memcache
# Starting rabbitmq-server: SUCCESS

# As we'll also want to run KVM virtual machines on this very same host, we'll need KVM and libvirt, which OpenStack uses to control virtual machines.
apt-get -y install kvm libvirt-bin
#Notice: nova-compute-kvm depends on kvm and libvirt-bin

# test virth
virsh -c qemu:///system list
# Id Name                 State
#----------------------------------

#????????????????????????????????????????????????????
#????????????????????????????????????????????????????
# nova-volume (optional)
df -h
#Filesystem      Size  Used Avail Use% Mounted on
#/dev/sda5       352G  7.4G  327G   3% /
#/dev/sda1       190M  128K  190M   1% /boot/efi
#/dev/sda2        93G  1.5G   87G   2% /nova-volume

umount /dev/sda2
pvcreate /dev/sda2
#  Physical volume "/dev/sda2" successfully created

vgcreate nova-volumes /dev/sda2
#  Volume group "nova-volumes" successfully created

vgscan
#  Reading all physical volumes.  This may take a while...
#  Found volume group "vg1" using metadata type lvm2
#  Found volume group "nova-volumes" using metadata type lvm2

# un comment /nova-volume so that this will not be mounted when restart
sed -i '/nova-volume/s/^/#/' /etc/fstab

#????????????????????????????????????????????????????
#????????????????????????????????????????????????????

# Export some envs that will be used later
#[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[
#[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[

vim novarc
#===================================================================
#===================================================================
export HOST_IP=$(/sbin/ifconfig eth0 | awk '/inet addr/ {print $2}' | cut -f2 -d ":")
export CONTROLLER_IP=192.168.1.188
export MYSQL_PASSWORD=rootpass
export ADMIN_TOKEN=KEZUNLIN
export ADMIN_PASSWORD=kezunlin
export OS_TENANT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=${ADMIN_PASSWORD}
export OS_AUTH_URL="http://${CONTROLLER_IP}:35357/v2.0/"
export OS_REGION_NAME=RegionOne
export SERVICE_TOKEN=${ADMIN_TOKEN}
export SERVICE_PASSWORD=${ADMIN_PASSWORD}
export SERVICE_ENDPOINT="http://${CONTROLLER_IP}:35357/v2.0"
#===================================================================
#===================================================================
cat novarc >> ~/.bashrc
source ~/.bashrc

env | grep OS_
env | grep ADMIN_
env | grep SERVICE_
env | grep CONTROLLER_IP
env | grep HOST_IP
env | grep MYSQL_PASSWORD

#root@controller:~# env | grep OS_
OS_REGION_NAME=RegionOne
OS_PASSWORD=kezunlin
OS_AUTH_URL=http://192.168.1.188:35357/v2.0/
OS_USERNAME=admin
OS_TENANT_NAME=admin
#root@controller:~# env | grep ADMIN_
ADMIN_TOKEN=KEZUNLIN
ADMIN_PASSWORD=kezunlin
#root@controller:~# env | grep SERVICE_
SERVICE_PASSWORD=kezunlin
SERVICE_ENDPOINT=http://192.168.1.188:35357/v2.0
SERVICE_TOKEN=KEZUNLIN
#root@controller:~# env | grep CONTROLLER_IP
CONTROLLER_IP=192.168.1.188
#root@controller:~# env | grep HOST_IP
HOST_IP=192.168.1.188
#root@controller:~# env | grep MYSQL_PASSWORD
MYSQL_PASSWORD=rootpass
#]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]
#]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]

#====================================================
#Step 2: Install MySQL and create the necessary databases and users
#====================================================
# Nova and glance will use MySQL to store their runtime data. To make sure they can do that, we'll install and set up MySQL
apt-get install -y mysql-server python-mysqldb
# (root,rootpass) for mysql

# Allow other machines to be able to talk to that MySQL database
sed -i 's/127.0.0.1/0.0.0.0/g' /etc/mysql/my.cnf  
service mysql restart

# show databases
mysql -u root -p$MYSQL_PASSWORD <<EOF
show databases;
EOF

# show users
mysql -u root -p$MYSQL_PASSWORD 
use mysql;
select Host,User,Password from user;
exit

#====================================================
# Step 3: Install and configure Keystone
#====================================================

#********************************************************
# keystone
mysql -u root -p$MYSQL_PASSWORD <<EOF
DROP DATABASE IF EXISTS keystone;
CREATE DATABASE keystone;
GRANT ALL PRIVILEGES ON keystone.* TO 'keystonedbadmin'@'%' IDENTIFIED BY 'keystonedbpass' WITH GRANT OPTION;
GRANT ALL PRIVILEGES ON keystone.* TO 'keystonedbadmin'@'localhost' IDENTIFIED BY 'keystonedbpass' WITH GRANT OPTION;
FLUSH PRIVILEGES;
EOF
#********************************************************

# We can finally get to OpenStack now and we'll start by installing the Identity component, codenamed Keystone. Install the according packages:
apt-get -y install keystone python-keystone python-keystoneclient

# remove keystone.db
rm -f /var/lib/keystone/keystone.db

# configure keystone
vim /etc/keystone/keystone.conf
#****************************************************************
#****************************************************************
#admin_token= ADMIN
admin_token = KEZUNLIN

[sql]
#connection = sqlite:////var/lib/keystone/keystone.db
connection = mysql://keystonedbadmin:keystonedbpass@192.168.1.188/keystone
#****************************************************************
#****************************************************************

# restart keystone
service keystone restart

#Then make Keystone create its tables within the freshly created keystone database:
keystone-manage db_sync

# show all tables in keystone
mysql -u root -p$MYSQL_PASSWORD <<EOF
use keystone;
show tables;
EOF

# The next step is to fill Keystone with actual data. 
# (1)Import data to keystone
wget http://www.hastexo.com/system/files/user/4/keystone_data.sh_.txt
mv keystone_data.sh_.txt keystone_data.sh
chmod +x keystone_data.sh

vim keystone_data.sh
#**************************************************
#**************************************************
#ADMIN_PASSWORD=${ADMIN_PASSWORD:-hastexo}
ADMIN_PASSWORD=${ADMIN_PASSWORD:-kezunlin}
#export SERVICE_TOKEN="hastexo"
export SERVICE_TOKEN="KEZUNLIN"
#**************************************************
#**************************************************
# SERVICE_TOKEN should be same as "admin_token = KEZUNLIN" in keystone_data.conf

./keystone_data.sh
# ./keystone_data.sh; echo $?
#if everything goes well, it should deliver a return code of 0
echo $?
#0

# select from keystone db
mysql -u root -p$MYSQL_PASSWORD <<EOF
use keystone;
select id,name from user;
select id,name from tenant;
select id,name from role;
EOF

keystone
#Optional arguments:
#  --os_username <auth-user-name>
#                        Defaults to env[OS_USERNAME]
#  --os_password <auth-password>
#                        Defaults to env[OS_PASSWORD]
#  --os_tenant_name <auth-tenant-name>
#                        Defaults to env[OS_TENANT_NAME]
#  --os_tenant_id <tenant-id>
#                        Defaults to env[OS_TENANT_ID]
#  --os_auth_url <auth-url>
#                        Defaults to env[OS_AUTH_URL]
#  --os_region_name <region-name>
#                        Defaults to env[OS_REGION_NAME]
#  --os_identity_api_version <identity-api-version>
#                        Defaults to env[OS_IDENTITY_API_VERSION] or 2.0
#  --token <service-token>
#                        Defaults to env[SERVICE_TOKEN]
#  --endpoint <service-endpoint>
#                        Defaults to env[SERVICE_ENDPOINT]

#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
# http://docs.openstack.org/folsom/openstack-compute/install/apt/content/verifying-identity-install.html
# Reminder
# Unlike basic authentication/authorization, which can be performed against either port 5000 or 35357, administrative commands MUST be performed against the admin API port: 35357). This means that you MUST use port 35357 in your OS_AUTH_URL or --os-auth-url setting.

# if os_auth_url use port 5000 instead of 35357,we will get an error when issue the following command:
keystone --os_tenant_name=admin --os_username=admin --os_password=kezunlin  --os_auth_url=http://127.0.0.1:5000/v2.0 user-list
#No handlers could be found for logger "keystoneclient.v2_0.client"
#Unable to communicate with identity service: 404 Not Found
#
#The resource could not be found.
#
#   . (HTTP 404)
#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!


# now test keystone
keystone --os_tenant_name=admin --os_username=admin --os_password=kezunlin  --os_auth_url=http://127.0.0.1:35357/v2.0 user-list

keystone user-list
keystone tenant-list
keystone role-list

# Last but not least, you'll also want to define endpoints in Keystone
# (2)Import endpoints to keystone
wget http://www.hastexo.com/system/files/user/4/endpoints.sh__0.txt
mv endpoints.sh__0.txt endpoints.sh
mod +x endpoints.sh

./endpoints.sh -h
#Usage: ./endpoints.sh [-m mysql_hostname] [-u mysql_username] [-D mysql_database] [-p mysql_password]
#       [-K keystone_master ] [ -R keystone_region ] [ -E keystone_endpoint_url ] 
#       [ -S swift_master ] [ -T keystone_token ]

./endpoints.sh -m 192.168.1.188 -u keystonedbadmin -D keystone -p keystonedbpass -K 192.168.1.188 -R RegionOne -E "http://localhost:35357/v2.0" -S 192.168.1.188 -T KEZUNLIN

# or
vim endpoints.sh
#**************************************************
#**************************************************
MYSQL_HOST=192.168.1.188
MYSQL_DATABASE=keystone
MYSQL_USER=keystonedbadmin
MYSQL_PASSWORD=keystonedbpass
MASTER=192.168.1.188
KEYSTONE_REGION=RegionOne
SWIFT_MASTER=192.168.1.188
#export SERVICE_ENDPOINT="http://192.168.1.188:35357/v2.0" 
#export SERVICE_TOKEN=KEZUNLIN
#**************************************************
#**************************************************
./endpoints.sh

# select from keystone db
mysql -u root -p$MYSQL_PASSWORD <<EOF 
use keystone;
select id,type,extra from service;
EOF

# test keystone
keystone service-list
keystone endpoint-list

# how to test keystone with curl
curl -d '{"auth": {"tenantName": "admin", "passwordCredentials":{"username": "admin", "password": "kezunlin"}}}' -H "Content-type: application/json" http://127.0.0.1:35357/v2.0/tokens | python -mjson.tool

#====================================================
#Step 4: Install and configure Glance
#====================================================
#********************************************************
# glance
mysql -u root -p$MYSQL_PASSWORD <<EOF
DROP DATABASE IF EXISTS glance;
CREATE DATABASE glance;
GRANT ALL PRIVILEGES ON glance.* TO 'glancedbadmin'@'%' IDENTIFIED BY 'glancedbpass' WITH GRANT OPTION;
GRANT ALL PRIVILEGES ON glance.* TO 'glancedbadmin'@'localhost' IDENTIFIED BY 'glancedbpass' WITH GRANT OPTION;
FLUSH PRIVILEGES;
EOF
#********************************************************

apt-get -y install glance glance-api glance-client glance-common glance-registry python-glance

# remove glance db
rm -f /var/lib/glance/glance.sqlite


vim /etc/glance/glance-api-paste.ini 
# use variables defined in keystone_data.sh
#**************************************************
#**************************************************
#admin_tenant_name = %SERVICE_TENANT_NAME% 
#admin_user = %SERVICE_USER% 
#admin_password = %SERVICE_PASSWORD%
admin_tenant_name = service
admin_user = glance
admin_password = kezunlin
#**************************************************
#**************************************************

vim /etc/glance/glance-registry-paste.ini 
#**************************************************
#**************************************************
#admin_tenant_name = %SERVICE_TENANT_NAME% 
#admin_user = %SERVICE_USER% 
#admin_password = %SERVICE_PASSWORD%
admin_tenant_name = service
admin_user = glance
admin_password = kezunlin
#**************************************************
#**************************************************

vim /etc/glance/glance-registry.conf
#**************************************************
#**************************************************
#sql_connection = sqlite:////var/lib/glance/glance.sqlite
sql_connection = mysql://glancedbadmin:glancedbpass@192.168.1.188/glance
[paste_deploy]
flavor = keystone
#**************************************************
#**************************************************
#Notice: It's important to use the machine's actual IP in this example and not 127.0.0.1!  For example, my ip is 192.168.1.188
#[paste_deploy]
#flavor = keystone
# These two lines instruct the Glance Registry to use Keystone for authentication, which is what we want.

vim /etc/glance/glance-api.conf
#**************************************************
#**************************************************
[paste_deploy]
flavor = keystone
#**************************************************
#**************************************************

# Afterwards, you need to initially synchronize the Glance database by running these commands:
glance-manage version_control 0
glance-manage db_sync         
# /usr/lib/python2.7/dist-packages/glance/registry/db/migrate_repo/versions/003_add_disk_format.py:47: SADeprecationWarning: useexisting is deprecated.  Use extend_existing.useexisting=True)

#It's time to restart Glance now:
service glance-api restart && service glance-registry restart

# Now what's the best method to verify that Glance is working as expected? The glance command line utilty can do that for us, but to work properly, it needs to know how we want to authenticate ourselves to Glance (and keystone, subsequently). This is a very good moment to define four environmental variables that we'll need continously when working with OpenStack: OS_TENANT_NAME, OS_USERNAME, OS_PASSWORD and OS_AUTH_URL.  

# verify that Glance is working as expected
# make sure OS_ has been exported
source novarc
env | grep OS_
#OS_PASSWORD=kezunlin
#OS_AUTH_URL=http://localhost:5000/v2.0/
#OS_USERNAME=admin
#OS_TENANT_NAME=admin

# After exporting these variables, you should be able to do:
glance index
echo $?
#0
# get no output at all in return (but the return code will be 0; check with echo $?). If that's the case, Glance is setup correctly and properly connects with Keystone. 

glance --version
#glance 2012.1.3-dev

# Now let's add our first image! 

# Upload images
# disk format :aki ami ari  iso raw vmdk
# container format: aki ami ari ovf bare
# Mapping disk_format-container_format
# raw-bare vmdk-bare iso-bare aki-aki ari-ari ami-ami

#==================================
# Test 2): download and test ubuntu 12.04 disk (220M)
# ubuntu 12.04 server amd64¿can be only login with ssh-key
#==================================
# http://cloud-images.ubuntu.com/
# http://cloud-images.ubuntu.com/precise/current/
# http://uec-images.ubuntu.com/releases/
# http://uec-images.ubuntu.com/releases/precise/release/
# download it from offical site, it may take a long time
wget http://192.168.1.184/openstack/ubuntu-12.04-server-cloudimg-amd64-disk1.img

# Then add this image to Glance:
glance add name="Ubuntu 12.04 server cloudimg amd64" is_public=true disk_format=qcow2 container_format=ovf  < ./image/ubuntu-12.04-server-cloudimg-amd64-disk1.img
#Added new image with ID: a3f0131a-8a44-4d08-ad52-f54634ddeaa3

#==================================
# Test 3): download and test cirros (10M)
# Cirros¿can be login with username and password or with ssh-key
#user:cirros
#password:cubswin:)
#==================================
#wget https://launchpad.net/cirros/trunk/0.3.0/+download/cirros-0.3.0-x86_64-disk.img
wget http://192.168.1.184/openstack/cirros-0.3.0-x86_64-disk.img

# Then add this image to Glance:
glance add name="cirros-0.30-x86_64" is_public=true disk_format=qcow2 container_format=bare  < ./image/cirros-0.3.0-x86_64-disk.img
# Added new image with ID: 43e71758-a376-409c-8fb7-7050f89ddb6a

# After this, you should be seeing the freshly added image.
glance index
#ID                                   Name                           Disk Format          Container Format     Size          
#------------------------------------ ------------------------------ -------------------- -------------------- --------------
#43e71758-a376-409c-8fb7-7050f89ddb6a cirros-0.30-x86_64             qcow2                bare                        9761280
#a3f0131a-8a44-4d08-ad52-f54634ddeaa3 Ubuntu 12.04 server cloudimg a qcow2                ovf                       254738432

#====================================================
# Step 5: Install and configure Nova
#====================================================
#********************************************************
# nova 
mysql -u root -p$MYSQL_PASSWORD <<EOF
DROP DATABASE IF EXISTS nova;
CREATE DATABASE nova;
GRANT ALL PRIVILEGES ON nova.* TO 'novadbadmin'@'%' IDENTIFIED BY 'novadbpass' WITH GRANT OPTION;
GRANT ALL PRIVILEGES ON nova.* TO 'novadbadmin'@'localhost' IDENTIFIED BY 'novadbpass' WITH GRANT OPTION;
FLUSH PRIVILEGES;
EOF
#********************************************************
apt-get -y install nova-api nova-cert nova-common nova-compute nova-compute-kvm nova-doc nova-network nova-objectstore nova-scheduler nova-volume nova-consoleauth novnc python-nova python-novaclient 

#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
./show_iptables
#========================================================
#filter
#-A nova-api-INPUT -d 192.168.1.188/32 -p tcp -m tcp --dport 8775 -j ACCEPT
#========================================================
#nat
#-A nova-network-POSTROUTING -s 10.0.0.0/8 -d 192.168.1.188/32 -j ACCEPT
#-A nova-network-POSTROUTING -s 10.0.0.0/8 -d 10.128.0.0/24 -j ACCEPT
#-A nova-network-POSTROUTING -s 10.0.0.0/8 -d 10.0.0.0/8 -m conntrack ! --ctstate DNAT -j ACCEPT
#-A nova-network-PREROUTING -d 169.254.169.254/32 -p tcp -m tcp --dport 80 -j DNAT --to-destination 192.168.1.188:8775
#-A nova-network-snat -j nova-network-float-snat
#-A nova-network-snat -s 10.0.0.0/8 -o eth0 -j SNAT --to-source 192.168.1.188
#========================================================

route -n
#Kernel IP routing table
#Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
#0.0.0.0         192.168.1.1     0.0.0.0         UG    100    0        0 eth0
#192.168.1.0     0.0.0.0         255.255.255.0   U     0      0        0 eth0
ip a
#1: lo: <LOOPBACK,UP,LOWER_UP> mtu 16436 qdisc noqueue state UNKNOWN 
#    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
#    inet 127.0.0.1/8 scope host lo
#    inet 169.254.169.254/32 scope link lo
#    inet6 ::1/128 scope host 
#       valid_lft forever preferred_lft forever
#2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000
#    link/ether 5c:f3:fc:b8:64:18 brd ff:ff:ff:ff:ff:ff
#    inet 192.168.1.188/24 brd 192.168.1.255 scope global eth0
#    inet6 fe80::5ef3:fcff:feb8:6418/64 scope link 
#       valid_lft forever preferred_lft forever
#3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000
#    link/ether 5c:f3:fc:b8:64:1a brd ff:ff:ff:ff:ff:ff
#    inet6 2001:da8:201:1017:784c:f768:13ea:66f4/64 scope global temporary dynamic 
#       valid_lft 603828sec preferred_lft 84828sec
#    inet6 2001:da8:201:1017:5ef3:fcff:feb8:641a/64 scope global dynamic 
#       valid_lft 2591830sec preferred_lft 604630sec
#    inet6 fe80::5ef3:fcff:feb8:641a/64 scope link 
#       valid_lft forever preferred_lft forever
#4: usb0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN qlen 1000
#    link/ether 5e:f3:fc:be:64:1b brd ff:ff:ff:ff:ff:ff
#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

# remove nova db
rm -f /var/lib/nova/nova.sqlite

# change user group of nova
chown -R nova:nova /etc/nova

vim /etc/nova/api-paste.ini
#**************************************************
#**************************************************
#admin_tenant_name = %SERVICE_TENANT_NAME% 
#admin_user = %SERVICE_USER% 
#admin_password = %SERVICE_PASSWORD%
admin_tenant_name = service
admin_user = nova
admin_password = kezunlin
#**************************************************
#**************************************************

vim /etc/nova/nova-compute.conf
#**************************************************
#**************************************************
--libvirt_type=kvm
#**************************************************
#**************************************************

vim /etc/nova/nova.conf
#**************************************************
#**************************************************
# ...
#**************************************************
#**************************************************

# Then, restart all nova services to make the configuration file changes take effect:
vim restart_nova.sh
#**************************************************
#**************************************************
#!/bin/bash
for a in libvirt-bin nova-network nova-compute nova-cert nova-api nova-objectstore nova-scheduler nova-volume novnc nova-consoleauth; do service "$a" stop; done

for a in libvirt-bin nova-network nova-compute nova-cert nova-api nova-objectstore nova-scheduler nova-volume novnc nova-consoleauth; do service "$a" start; done
#**************************************************
#**************************************************

chmod +x restart_nova.sh 
./restart_nova.sh

#====================================================
#libvirt-bin stop/waiting
#nova-network stop/waiting
#nova-compute stop/waiting
#nova-cert stop/waiting
#nova-api stop/waiting
#nova-objectstore stop/waiting
#nova-scheduler stop/waiting
#nova-volume stop/waiting
# * Stopping OpenStack NoVNC proxy nova-novncproxy                        [ OK ] 
#nova-consoleauth stop/waiting
#libvirt-bin start/running, process 26516
#nova-network start/running, process 26536
#nova-compute start/running, process 26547
#nova-cert start/running, process 26557
#nova-api start/running, process 26569
#nova-objectstore start/running, process 26585
#nova-scheduler start/running, process 26600
#nova-volume start/running, process 26615
# * Starting OpenStack NoVNC proxy  nova-novncproxy                       [ OK ] 
#nova-consoleauth start/running, process 26663
#====================================================

#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
./show_iptables
#========================================================
#filter
#-A nova-api-INPUT -d 192.168.1.188/32 -p tcp -m tcp --dport 8775 -j ACCEPT
#========================================================
#nat
-A nova-network-POSTROUTING -s 192.168.2.192/27 -d 192.168.1.188/32 -j ACCEPT
-A nova-network-POSTROUTING -s 192.168.2.192/27 -d 10.128.0.0/24 -j ACCEPT
-A nova-network-POSTROUTING -s 192.168.2.192/27 -d 192.168.2.192/27 -m conntrack ! --ctstate DNAT -j ACCEPT
-A nova-network-PREROUTING -d 169.254.169.254/32 -p tcp -m tcp --dport 80 -j DNAT --to-destination 192.168.1.188:8775
-A nova-network-snat -j nova-network-float-snat
-A nova-network-snat -s 192.168.2.192/27 -o eth0 -j SNAT --to-source 192.168.1.188
#========================================================
route -n
ip a
#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

# The next step will create all tables Nova needs in MySQL. While we are at it, we can also create the network we want to use for our VMs in the Nova databases. Do this:
nova-manage db sync
# 2013-11-20 21:45:26 DEBUG nova.utils [-] backend <module 'nova.db.sqlalchemy.migration' from '/usr/lib/python2.7/dist-packages/nova/db/sqlalchemy/migration.pyc'> from (pid=27219) __get_backend /usr/lib/python2.7/dist-packages/nova/utils.py:663
#2013-11-20 21:45:31 WARNING nova.utils [-] /usr/lib/python2.7/dist-packages/sqlalchemy/pool.py:639: SADeprecationWarning: The 'listeners' argument to Pool (and create_engine()) is deprecated.  Use event.listen().
#  Pool.__init__(self, creator, **kw)
#
#2013-11-20 21:45:31 WARNING nova.utils [-] /usr/lib/python2.7/dist-packages/sqlalchemy/pool.py:145: SADeprecationWarning: Pool.add_listener is deprecated.  Use event.listen()
#  self.add_listener(l)
#
#2013-11-20 21:45:31 AUDIT nova.db.sqlalchemy.fix_dns_domains [-] Applying database fix for Essex dns_domains table.

# select from nova db
mysql -u root -p$MYSQL_PASSWORD <<EOF 
use nova;
show tables;
EOF

#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
./show_iptables
route -n
ip a
#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

# create fixed ips
#nova-manage network create private --fixed_range_v4=192.168.2.192/27 --num_networks=1 --bridge=br100 --bridge_interface=eth1 --network_size=32 --multi_host=T
nova-manage network create private --fixed_range_v4=192.168.2.192/27 --num_networks=1 --bridge=br100 --bridge_interface=eth1 --network_size=32 
# create 1 record in table 'networks'
# create 32 records in table 'fixed_ips'

#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
./show_iptables
route -n
ip a
#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

mysql -u root -p$MYSQL_PASSWORD nova <<EOF
select id,cidr from networks;
EOF
#id	cidr
#1	192.168.2.192/27

mysql -u root -prootpass nova <<EOF
select id,address,network_id from fixed_ips;
EOF

# restart nova again
./restart_nova.sh

mysql -u root -p$MYSQL_PASSWORD nova 
select id,host,disabled,topic,availability_zone from services;
exit
#+----+------------+----------+-------------+-------------------+
#| id | host       | disabled | topic       | availability_zone |
#+----+------------+----------+-------------+-------------------+
#|  1 | controller |        0 | consoleauth | nova              |
#|  2 | controller |        0 | scheduler   | nova              |
#|  3 | controller |        0 | compute     | nova              |
#|  4 | controller |        0 | cert        | nova              |
#|  5 | controller |        0 | volume      | nova              |
#|  6 | controller |        0 | network     | nova              |
#+----+------------+----------+-------------+-------------------+

nova-manage service list
#Binary           Host                                 Zone             Status     State Updated_At
#nova-consoleauth ubuntu                               nova             enabled    :-)   2013-11-26 03:49:16
#nova-scheduler   ubuntu                               nova             enabled    :-)   2013-11-26 03:49:15
#nova-compute     ubuntu                               nova             enabled    :-)   2013-11-26 03:49:13
#nova-cert        ubuntu                               nova             enabled    :-)   2013-11-26 03:49:16
#nova-volume      ubuntu                               nova             enabled    :-)   2013-11-26 03:49:16
#nova-network     ubuntu                               nova             enabled    :-)   2013-11-26 03:49:16


# list all currently running VMs (none, the list should be empty
nova list

# list all images uploaded to glance
nova image-list

#====================================================
# Step 6: Your first VM
#====================================================
# Once Nova works as desired, starting your first own cloud VM is easy

ssh-keygen
#Generating public/private rsa key pair.
#Enter file in which to save the key (/root/.ssh/id_rsa): 
#Enter passphrase (empty for no passphrase): 
#Enter same passphrase again: 
#Your identification has been saved in /root/.ssh/id_rsa.
#Your public key has been saved in /root/.ssh/id_rsa.pub.
#The key fingerprint is:
#84:e0:6f:12:6b:51:2d:81:d0:15:59:2b:c2:cb:7f:0d root@ubuntu

nova keypair-add --pub_key /root/.ssh/id_rsa.pub key1
#nova keypair-delete key1

#This will add the key to OpenStack Nova and store it with the name "key1". The only thing left to do after this is firing up your VM. 

nova keypair-list
+------+-------------------------------------------------+
| Name |                   Fingerprint                   |
+------+-------------------------------------------------+
| key1 | 98:1f:be:5c:33:38:4d:d5:60:7c:b9:7c:9d:fc:c3:f8 |
+------+-------------------------------------------------+

#Find out what ID your Ubuntu image has, you can do this with:
nova image-list
+--------------------------------------+------------------------------------+--------+--------+
|                  ID                  |                Name                | Status | Server |
+--------------------------------------+------------------------------------+--------+--------+
| 43e71758-a376-409c-8fb7-7050f89ddb6a | cirros-0.30-x86_64                 | ACTIVE |        |
| a3f0131a-8a44-4d08-ad52-f54634ddeaa3 | Ubuntu 12.04 server cloudimg amd64 | ACTIVE |        |
+--------------------------------------+------------------------------------+--------+--------+

# When starting a VM, you also need to define the flavor it is supposed to use. Flavors are pre-defined hardware schemes in OpenStack with which you can define what resources your newly created VM has. OpenStack comes with five pre-defined flavors; you can get an overview over the existing flavors with
nova flavor-list
+----+-----------+-----------+------+-----------+------+-------+-------------+
| ID |    Name   | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor |
+----+-----------+-----------+------+-----------+------+-------+-------------+
| 1  | m1.tiny   | 512       | 0    | 0         |      | 1     | 1.0         |
| 2  | m1.small  | 2048      | 10   | 20        |      | 1     | 1.0         |
| 3  | m1.medium | 4096      | 10   | 40        |      | 2     | 1.0         |
| 4  | m1.large  | 8192      | 10   | 80        |      | 4     | 1.0         |
| 5  | m1.xlarge | 16384     | 10   | 160       |      | 8     | 1.0         |
+----+-----------+-----------+------+-----------+------+-------+-------------+

#Flavors are referenced by their ID, not by  their name. That's important for the actual command to execute to start your VM. That command's syntax basically is this:
#nova boot --flavor ID --image Image-UUID --key_name key-name vm_name

#=======================================================================
#=======================================================================
root@controller:~# route -n
root@controller:~# ifconfig
root@controller:~# ip a
#=======================================================================
#=======================================================================

# let us use cirros, its image id is 43e71758-a376-409c-8fb7-7050f89ddb6a
nova boot --flavor 1 --image 43e71758-a376-409c-8fb7-7050f89ddb6a --key_name key1 vm1 

#nova delete vmID
#nova delete 14c9285d-7709-4594-9f53-df0eb7c5a274  

nova show vm1
# id=999292bc-a25e-4493-b9ad-0ca864049afe 
# status=BUILD---->ACTIVE
# private network =192.168.2.194 

# nova pause | unpause | resume | reboot | boot | resize |
nova list
+--------------------------------------+------+--------+-----------------------+
|                  ID                  | Name | Status |        Networks       |
+--------------------------------------+------+--------+-----------------------+
| 55e06522-4586-44aa-b29d-a3594c6591be | vm1  | ACTIVE | private=192.168.2.194 |
+--------------------------------------+------+--------+-----------------------+

virsh list
# Id Name                 State
#----------------------------------
#  1 instance-00000001    running

mysql -u root -p$MYSQL_PASSWORD nova <<EOF
select id,display_name from instances;
EOF
#id	display_name
#1	vm1
#2	ubuntu1

#=======================================================================
#=======================================================================
root@controller:~# route -n
#Kernel IP routing table
#Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
#0.0.0.0         192.168.1.1     0.0.0.0         UG    100    0        0 eth0
#192.168.1.0     0.0.0.0         255.255.255.0   U     0      0        0 eth0
#192.168.2.0     0.0.0.0         255.255.255.0   U     0      0        0 br100
#192.168.2.192   0.0.0.0         255.255.255.224 U     0      0        0 br100
#192.168.122.0   0.0.0.0         255.255.255.0   U     0      0        0 virbr0
root@controller:~# ifconfig
#br100     Link encap:Ethernet  HWaddr 5c:f3:fc:b8:64:1a  
#          inet addr:192.168.2.193  Bcast:192.168.2.223  Mask:255.255.255.224
#          inet6 addr: 2001:da8:201:1017:5ef3:fcff:feb8:641a/64 Scope:Global
#          inet6 addr: 2001:da8:201:1017:244f:46e4:b516:4119/64 Scope:Global
#          inet6 addr: fe80::7cdd:d8ff:fea2:3cee/64 Scope:Link
#          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
#          RX packets:131 errors:0 dropped:0 overruns:0 frame:0
#          TX packets:8 errors:0 dropped:0 overruns:0 carrier:0
#          collisions:0 txqueuelen:0 
#          RX bytes:13022 (13.0 KB)  TX bytes:704 (704.0 B)
#
#eth0      Link encap:Ethernet  HWaddr 5c:f3:fc:b8:64:18  
#          inet addr:192.168.1.188  Bcast:192.168.1.255  Mask:255.255.255.0
#          inet6 addr: fe80::5ef3:fcff:feb8:6418/64 Scope:Link
#          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
#          RX packets:64288 errors:0 dropped:0 overruns:0 frame:0
#          TX packets:37293 errors:0 dropped:0 overruns:0 carrier:0
#          collisions:0 txqueuelen:1000 
#          RX bytes:74382849 (74.3 MB)  TX bytes:3411398 (3.4 MB)
#          Interrupt:28 Memory:92000000-92012800 
#
#eth1      Link encap:Ethernet  HWaddr 5c:f3:fc:b8:64:1a  
#          inet6 addr: 2001:da8:201:1017:5ef3:fcff:feb8:641a/64 Scope:Global
#          inet6 addr: fe80::5ef3:fcff:feb8:641a/64 Scope:Link
#          inet6 addr: 2001:da8:201:1017:ce5:b78f:2206:35d7/64 Scope:Global
#          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
#          RX packets:33378 errors:0 dropped:0 overruns:0 frame:0
#          TX packets:204 errors:0 dropped:0 overruns:0 carrier:0
#          collisions:0 txqueuelen:1000 
#          RX bytes:4093380 (4.0 MB)  TX bytes:19834 (19.8 KB)
#          Interrupt:40 Memory:94000000-94012800 
#
#lo        Link encap:Local Loopback  
#          inet addr:127.0.0.1  Mask:255.0.0.0
#          inet6 addr: ::1/128 Scope:Host
#          UP LOOPBACK RUNNING  MTU:16436  Metric:1
#          RX packets:35807 errors:0 dropped:0 overruns:0 frame:0
#          TX packets:35807 errors:0 dropped:0 overruns:0 carrier:0
#          collisions:0 txqueuelen:0 
#          RX bytes:279448848 (279.4 MB)  TX bytes:279448848 (279.4 MB)
#
#virbr0    Link encap:Ethernet  HWaddr b6:ca:d9:f2:19:a4  
#          inet addr:192.168.122.1  Bcast:192.168.122.255  Mask:255.255.255.0
#          UP BROADCAST MULTICAST  MTU:1500  Metric:1
#          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
#          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
#          collisions:0 txqueuelen:0 
#          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)
#=======================================================================
#=======================================================================

# Now it's time to access our first vm.
#user:cirros
#password:cubswin:)
ssh cirros@192.168.2.194
# OR
ssh -i /root/.ssh/id_rsa cirros@192.168.2.194

root@master:~/ke/guides/openstack# ssh cirros@192.168.2.194
#$ exit
#Connection to 192.168.2.194 closed.

# scp file
scp 1.txt cirros@192.168.2.194:/home/cirros/

# ping vm
ping 192.168.2.194

# ubuntu server vm
nova boot --flavor 2 --image a3f0131a-8a44-4d08-ad52-f54634ddeaa3 --key_name key1 ubuntu1
nova list
ssh -i /root/.ssh/id_rsa ubuntu@192.168.2.195
#ubuntu@ubuntu1:~$ sudo passwd root
#Enter new UNIX password: 
#Retype new UNIX password: 
#passwd: password updated successfully

#==============================================================
# view nova db
mysql -u root
use nova;
select id,address,instance_id from fixed_ips;
select id,address,fixed_ip_id from floating_ips;

select id from instances;
select id,instance_id,network_info from instance_info_caches;
describe security_group_instance_association;
select id,security_group_id,instance_id from security_group_instance_association;
select id,name from security_groups;

# how to delete services
delete from services where host='ubuntu';
delete from services where host='slave';

# how to delete instances
SET FOREIGN_KEY_CHECKS=0;
delete from instances where id = '29';
delete from instances where id = '30';
SET FOREIGN_KEY_CHECKS=1;
#==============================================================

#====================================================
# Step 7: The OpenStack Dashboard
#====================================================
# We can use Nova to start and stop virtual machines now, but up to this point, we can only do it on the command line. That's not good, because typically, we'll want users without high-level administrator skills to be able to start new VMs. There's a solution for this on the OpenStack ecosystem called Dashboard, codename Horizon. Horizon is OpenStack's main configuration interface. It's django-based.

# Dashboard
apt-get -y install apache2 libapache2-mod-wsgi openstack-dashboard

service apache2 restart
# After this, point your webbrowser to the Nova machine's IP address and you should see the OpenStack Dashboard login prompt. Login with admin and the password you specified. That's it - you're in!

wget 192.168.1.188:80
# (admin,kezunlin)

#====================================================
# Step 8: Using floating IPs
#====================================================
# Floating IPs are an unbelievably handy tool in OpenStack to supply your virtual machines with "official" IP addresses. In this example, we've mainly been dealing with the 192.168.22.0/24 network, which is the "internal" network for our VMs. Our VMs can communicate with each other and they can communicate with the outside world, but they don't have an official IP address that others could connect to (the "public" net in this test-setup is 10.42.0.0/24 after all). Floating IPs allow you to assign your VMs an additional IP from that "public" network, making them accessible directly. And using floating IPs is anything but hard!

# First, you'll have to define a range of addresses which OpenStack nova will use. Our old friend nova-manage does this: 
nova-manage floating create --ip_range=192.168.1.192/27
# create records in nova.floating_ips

mysql -u root -p$MYSQL_PASSWORD nova <<EOF 
select id,address from floating_ips;
EOF

# Then, within Nova itself, you'll have to create a floating IP (creating here is Nova-speak for "reserving"):
nova floating-ip-create
# nova floating-ip-delete 192.168.1.33
+---------------+-------------+----------+------+
|       Ip      | Instance Id | Fixed Ip | Pool |
+---------------+-------------+----------+------+
| 192.168.1.193 | None        | None     | nova |
+---------------+-------------+----------+------+

# To assign this IP to our vm1, use this command: 
nova add-floating-ip vm1 192.168.1.193

# delete floating ip
#nova remove-floating-ip vm1 192.168.1.193

# Please note: Assigning a floating IP to an existing VM does automatically enable that IP for the VM. You'll not have to manually assign the IP to the VMs main network interface, as all the networking magic is done by iptables on the actual compute node.
#That's it! Your new VM can now use its floating IP. There is only one problem left: By default, nova uses very secure iptables rules to protect IPs reachable via floating IPs from abuse. De facto, nova will not allow any traffic from the outside to get through to your VM. We'll have to fiddle with Security Groups to solve this problem. Here's how you can enable SSH access and ICMP to your floating IPs:

# modify security group rule so that we can visit the vm1 by floating IP.
nova secgroup-add-rule default tcp 22 22 0.0.0.0/0
nova secgroup-add-rule default icmp -1 -1 0.0.0.0/0

nova secgroup-list
nova secgroup-list-rules default

# After this, your VM will be reachable directly from the outside via its floating IP address (by SSH and ICMP).

# Test
# Now go to another machine and ssh to vm1
# 1)user,password login mode
root@slave:~# ssh cirros@192.168.1.193
# input password cubswin:)
# 2)ssh-key login mode
root@slave:~# scp master:/root/.ssh/id_rsa id_rsa_33
root@slave:~# ssh -i id_rsa_33 cirros@192.168.1.193

# or ping vm1
root@slave:~# ping cirros@192.168.1.193
# OK

# vnc to vm
nova get-vnc-console vm1 novnc
+-------+------------------------------------------------------------------------------------+
|  Type |                                        Url                                         |
+-------+------------------------------------------------------------------------------------+
| novnc | http://192.168.1.188:6080/vnc_auto.html?token=9ead53c3-1d1b-4233-9963-37834bbdc6e3 |
+-------+------------------------------------------------------------------------------------+

# open your chrome, access http://192.168.1.188:6080/vnc_auto.html?token=9ead53c3-1d1b-4233-9963-37834bbdc6e3
# OK
# cirros,cubswin:)

#====================================================
# Appendix: Making nova-volume work
#====================================================
# nova-volume is the OpenStack Compute component that will allow you to assign persistent storage devices to your virtual machines. Internally, it's using iSCSI, which is why you installed the tgt package earlier.

# Assuming that you have a local LVM volume group entitled nova-volumes, you can try assigning a 1G large volume to our superfrobnicator VM by using these commands to create a 1G large volume and assign it accordingly: 
# create a 1G volume named volume1
nova volume-create --display_name "volume1" 1
# nova volume-delete 1

nova volume-list
#+----+-----------+--------------+------+-------------+-------------+
#| ID |   Status  | Display Name | Size | Volume Type | Attached to |
#+----+-----------+--------------+------+-------------+-------------+
#| 1  | available | volume1      | 1    | None        |             |
#+----+-----------+--------------+------+-------------+-------------+


ssh -i ~/.ssh/id_rsa cirros@192.168.1.193
# cat /proc/partitions 
major minor  #blocks  name

 253        0      40162 vda
 253        1      32130 vda1
# df -h
Filesystem                Size      Used Available Use% Mounted on
/dev                    243.1M         0    243.1M   0% /dev
/dev/vda1                23.2M     12.9M      9.1M  59% /
tmpfs                   246.1M         0    246.1M   0% /dev/shm
tmpfs                   188.0K     20.0K    180.0K  10% /run

# nova volume-attach server_name volume_id device
nova volume-attach vm1 1 /dev/vdb

# nova volume-detach vm1 1

nova volume-list
#+----+--------+--------------+------+-------------+--------------------------------------+
#| ID | Status | Display Name | Size | Volume Type |             Attached to              |
#+----+--------+--------------+------+-------------+--------------------------------------+
#| 1  | in-use | volume1      | 1    | None        | d89fdee9-e9c2-4f5f-8ae7-eb402be1f772 |
#+----+--------+--------------+------+-------------+--------------------------------------+

# If everything went well, you'll see a new disk device in the vm1 VM now, /dev/vdb. 

# cat /proc/partitions 
major minor  #blocks  name

 253        0      40162 vda
 253        1      32130 vda1
 253       16    1048576 vdb

# mkfs.ext3 /dev/vdb
# mount /dev/vdb /mnt
# df -h
Filesystem                Size      Used Available Use% Mounted on
/dev                    243.1M         0    243.1M   0% /dev
/dev/vda1                23.2M     12.9M      9.1M  59% /
tmpfs                   246.1M         0    246.1M   0% /dev/shm
tmpfs                   188.0K     20.0K    180.0K  10% /run
/dev/vdb               1007.9M     33.3M    923.4M   3% /mnt

# Now you can use /dev/vdb in vm2 VM.


# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
#****************************************************
# What need to do if modify hostname from ubuntu to controller
#****************************************************
vim /etc/hostname
#controller
vim /etc/hosts
#127.0.0.1       localhost
#192.168.1.188   controller.3d.pku.edu.cn        controller

reboot

# After modifing hostname from ubuntu to controller, you need to do
mysql -u root -prootpass mysql;
select Host,User,Password from user;
exit
#+-----------+------------------+-------------------------------------------+
#| Host      | User             | Password                                  |
#+-----------+------------------+-------------------------------------------+
#| localhost | root             | *3800D13EE735ED411CBC3F23B2A2E19C63CE0BEC |
#| ubuntu    | root             | *81F5E21E35407D884A6CD4A731AEBFB6AF209E1B |
#| 127.0.0.1 | root             | *81F5E21E35407D884A6CD4A731AEBFB6AF209E1B |
#| ::1       | root             | *81F5E21E35407D884A6CD4A731AEBFB6AF209E1B |
#| localhost |                  |                                           |
#| ubuntu    |                  |                                           |
#| localhost | debian-sys-maint | *E6AD1D34AF10018BDE798EB1F356116E51DD6BBE |
#| %         | novadbadmin      | *74012C0F692D9BEE7865E382DF8C09364D45B8A3 |
#| %         | glancedbadmin    | *98D03AA6601520B9968B8ECEFAAB148D73AA09FE |
#| %         | keystonedbadmin  | *EA941CA8E7E8F4C01C7E7E96A6982F73CF0A4046 |
#+-----------+------------------+-------------------------------------------+

mysql -u root -prootpass mysql;
update user set Host='controller' where Host='ubuntu';
select Host,User,Password from user;
exit
#+------------+------------------+-------------------------------------------+
#| Host       | User             | Password                                  |
#+------------+------------------+-------------------------------------------+
#| localhost  | root             | *3800D13EE735ED411CBC3F23B2A2E19C63CE0BEC |
#| controller | root             | *81F5E21E35407D884A6CD4A731AEBFB6AF209E1B |
#| 127.0.0.1  | root             | *81F5E21E35407D884A6CD4A731AEBFB6AF209E1B |
#| ::1        | root             | *81F5E21E35407D884A6CD4A731AEBFB6AF209E1B |
#| localhost  |                  |                                           |
#| controller |                  |                                           |
#| localhost  | debian-sys-maint | *E6AD1D34AF10018BDE798EB1F356116E51DD6BBE |
#| %          | novadbadmin      | *74012C0F692D9BEE7865E382DF8C09364D45B8A3 |
#| %          | glancedbadmin    | *98D03AA6601520B9968B8ECEFAAB148D73AA09FE |
#| %          | keystonedbadmin  | *EA941CA8E7E8F4C01C7E7E96A6982F73CF0A4046 |
#+------------+------------------+-------------------------------------------+
