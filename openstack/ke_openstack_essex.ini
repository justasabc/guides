# A quick guide to install openstack essex on ubuntu 12.04
#----------------------------------------------------------------------
# http://www.hastexo.com/resources/docs/installing-openstack-grizzly-20131-ubuntu-1204-precise-pangolin
# http://www.chenshake.com/ubuntu-12-04-openstack-essex-installation-single-node/#i-16
# http://www.chenshake.com/ubuntu12-04-2-installed-openstack-grizzly-bridge-mode/
# http://www.mirantis.com/blog/openstack-networking-flatmanager-and-flatdhcpmanager/
# http://blog.csdn.net/spch2008/article/details/7862950
#----------------------------------------------------------------------
# OpenStack Essex 2013.1.3

#====================================================
# Step 1: Prepare your System
#====================================================
apt-get update
apt-get -y upgrade
apt-get -y install bridge-utils


#By default, the IP forwarding is disabled on most of Linux distributions. The "floating IP" feature requires the IP forwarding enabled in order to work, you can check if the forwarding is enabled by running the following command:
#Enabling IP forwarding
echo 1 > /proc/sys/net/ipv4/ip_forward
cat /proc/sys/net/ipv4/ip_forward
#1

vim /etc/network/interfaces
#**************************************************
#**************************************************
# ...
#**************************************************
#**************************************************
/etc/init.d/networking restart

# install ntp and configure
apt-get -y install ntp
vim /etc/ntp.conf
server ntp.ubuntu.com iburst
server 127.127.1.0
fudge 127.127.1.0 stratum 10

service ntp restart
/etc/init.d/ntp restart
 
# install tgt target (we'll need it for nova-volume) 
apt-get -y install tgt
service tgt restart

# install iscsi client
apt-get -y install open-iscsi open-iscsi-utils

# We'll also need RabbitMQ, an AMQP-implementation, as that is what all OpenStack components use to communicate with eath other, and memcached.
apt-get -y install rabbitmq-server memcached python-memcache

# As we'll also want to run KVM virtual machines on this very same host, we'll need KVM and libvirt, which OpenStack uses to control virtual machines.
apt-get -y install kvm libvirt-bin

# test virth
virsh -c qemu:///system list
# Id Name                 State
#----------------------------------

# if virsh hang up
killall -9 dmidecode

#====================================================
#Step 2: Install MySQL and create the necessary databases and users
#====================================================
# Nova and glance will use MySQL to store their runtime data. To make sure they can do that, we'll install and set up MySQL
apt-get install -y mysql-server python-mysqldb
# Allow other machines to be able to talk to that MySQL database
sed -i 's/127.0.0.1/0.0.0.0/g' /etc/mysql/my.cnf  
service mysql restart

# Now create the user accounts in mysql and grant them access on the according databases
# nova 
mysql -u root -prootpass <<EOF
DROP DATABASE IF EXISTS nova;
CREATE DATABASE nova;
GRANT ALL PRIVILEGES ON nova.* TO 'novadbadmin'@'%' IDENTIFIED BY 'novadbpass' WITH GRANT OPTION;
FLUSH PRIVILEGES;
EOF

# glance
mysql -u root -prootpass <<EOF
DROP DATABASE IF EXISTS glance;
CREATE DATABASE glance;
GRANT ALL PRIVILEGES ON glance.* TO 'glancedbadmin'@'%' IDENTIFIED BY 'glancedbpass' WITH GRANT OPTION;
FLUSH PRIVILEGES;
EOF

# keystone
mysql -u root -prootpass <<EOF
DROP DATABASE IF EXISTS keystone;
CREATE DATABASE keystone;
GRANT ALL PRIVILEGES ON keystone.* TO 'keystonedbadmin'@'%' IDENTIFIED BY 'keystonedbpass' WITH GRANT OPTION;
FLUSH PRIVILEGES;
EOF

# show databases
mysql -u root -prootpass <<EOF 
show databases;
EOF
# nova glance keystone

# show users
mysql -u root -prootpass <<EOF 
use mysql;
select Host,User,Password from user;
EOF

#====================================================
# Step 3: Install and configure Keystone
#====================================================
# We can finally get to OpenStack now and we'll start by installing the Identity component, codenamed Keystone. Install the according packages:
apt-get -y install keystone python-keystone python-keystoneclient

# configure keystone
vim /etc/keystone/keystone.conf
#****************************************************************
#****************************************************************
#admin_token= ADMIN
admin_token = KEZUNLIN

[sql]
#connection = sqlite:////var/lib/keystone/keystone.db
connection = mysql://keystonedbadmin:keystonedbpass@192.168.1.200/keystone
idle_timeout = 200
#****************************************************************
#****************************************************************

# restart keystone
service keystone restart

#Then make Keystone create its tables within the freshly created keystone database:
keystone-manage db_sync

# show all tables in keystone
mysql -u root -prootpass <<EOF 
use keystone;
show tables;
EOF
#Tables_in_keystone
#ec2_credential
#endpoint
#metadata
#migrate_version
#role
#service
#tenant
#token
#user
#user_tenant_membership

# The next step is to fill Keystone with actual data. 
# (1)Import data to keystone
wget http://www.hastexo.com/system/files/user/4/keystone_data.sh_.txt
mv keystone_data.sh_.txt keystone_data.sh
chmod +x keystone_data.sh

vim keystone_data.sh
#**************************************************
#**************************************************
#ADMIN_PASSWORD=${ADMIN_PASSWORD:-hastexo}
ADMIN_PASSWORD=${ADMIN_PASSWORD:-kezunlin}
#export SERVICE_TOKEN="hastexo"
export SERVICE_TOKEN="KEZUNLIN"
#**************************************************
#**************************************************
# SERVICE_TOKEN should be same as "admin_token = KEZUNLIN" in keystone_data.conf

./keystone_data.sh
# ./keystone_data.sh; echo $?
#if everything goes well, it should deliver a return code of 0
echo $?
#0

# select from keystone db
mysql -u root -prootpass <<EOF 
use keystone;
select id,name from user;
select id,name from tenant;
select id,name from role;
EOF
#id	name
#5cea87080d1d4f02b2587489c0c0dd95	admin
#bb6492df522e4c9f8dda1052257f2f0b	demo
#7909371bcc914e04bd673f71dfcf7d0b	glance
#c4aa811f4a214abbb4026c827a2f163a	nova
#id	name
#fe949fccd7cb48629b67ae074a430a57	admin
#28637b7f5aab471395d716adb709949a	demo
#dc79a21135cd407ab225efb66ec7e7d6	invisible_to_admin
#5bd2e2444730444cbd1c615ac6db109a	service
#id	name
#cdf00dd87bf04138ac6a38450b209eff	admin
#a042f8d3a20a4db58ae830ed758dab82	anotherrole
#44a7a4f8036949fe8f6af6414a146632	KeystoneAdmin
#3da5646b8d184546a80c386deefbe07d	KeystoneServiceAdmin
#3af9330b9334448a8f83baa979bf0390	Member

keystone
#Optional arguments:
#  --os_username <auth-user-name>
#                        Defaults to env[OS_USERNAME]
#  --os_password <auth-password>
#                        Defaults to env[OS_PASSWORD]
#  --os_tenant_name <auth-tenant-name>
#                        Defaults to env[OS_TENANT_NAME]
#  --os_tenant_id <tenant-id>
#                        Defaults to env[OS_TENANT_ID]
#  --os_auth_url <auth-url>
#                        Defaults to env[OS_AUTH_URL]

#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
# http://docs.openstack.org/folsom/openstack-compute/install/apt/content/verifying-identity-install.html
# Reminder
# Unlike basic authentication/authorization, which can be performed against either port 5000 or 35357, administrative commands MUST be performed against the admin API port: 35357). This means that you MUST use port 35357 in your OS_AUTH_URL or --os-auth-url setting.

# if os_auth_url use port 5000 instead of 35357,we will get an error when issue the following command:
keystone --os_tenant_name=admin --os_username=admin --os_password=kezunlin  --os_auth_url=http://127.0.0.1:5000/v2.0 user-list
#No handlers could be found for logger "keystoneclient.v2_0.client"
#Unable to communicate with identity service: 404 Not Found
#
#The resource could not be found.
#
#   . (HTTP 404)
#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

vim novarc
#**************************************************
#**************************************************
export OS_TENANT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=kezunlin
#export OS_AUTH_URL="http://localhost:5000/v2.0/"
export OS_AUTH_URL="http://localhost:35357/v2.0/"
#**************************************************
#**************************************************

source novarc
cat novarc >> ~/.bashrc

# check whether OS_ has been exported successfully
env | grep OS_
#OS_PASSWORD=kezunlin
#OS_AUTH_URL=http://localhost:35357/v2.0/
#OS_USERNAME=admin
#OS_TENANT_NAME=admin

# now test keystone
keystone --os_tenant_name=admin --os_username=admin --os_password=kezunlin  --os_auth_url=http://127.0.0.1:35357/v2.0 user-list

keystone user-list
+----------------------------------+---------+--------------------+--------+
|                id                | enabled |       email        |  name  |
+----------------------------------+---------+--------------------+--------+
| 2b0d79fa05a74a2ca205294060e4fe18 | True    | nova@hastexo.com   | nova   |
| 7983720f688a41a1abf28d712c3b6f6e | True    | demo@hastexo.com   | demo   |
| 7b232a0fdefc46c4a1f127b094c8c1ff | True    | glance@hastexo.com | glance |
| f7f3454f91d542a9b691f1a24b1d8be8 | True    | admin@hastexo.com  | admin  |
+----------------------------------+---------+--------------------+--------+

keystone tenant-list
+----------------------------------+--------------------+---------+
|                id                |        name        | enabled |
+----------------------------------+--------------------+---------+
| 022da686eebd410186e35c0b2c4b27de | invisible_to_admin | True    |
| 6d5a5185e9c247debc4fd10377a4d42f | admin              | True    |
| e59d6ee354ce4cbf8be1f4838160ef66 | demo               | True    |
| fec1001aa3c540b6908bbba260e7381d | service            | True    |
+----------------------------------+--------------------+---------+

keystone role-list
+----------------------------------+----------------------+
|                id                |         name         |
+----------------------------------+----------------------+
| 40b0d25272b54196949a37af81da1842 | KeystoneAdmin        |
| 9bf30509ccb749ddb206e6072391099c | anotherrole          |
| c9676be8f8a2425f8556ff74c841fc20 | KeystoneServiceAdmin |
| f5e2f086f1e643c494e6e94455c90a96 | Member               |
| fef3f58086e447a2abff88490eba4e62 | admin                |
+----------------------------------+----------------------+

keystone service-list
+----+------+------+-------------+
| id | name | type | description |
+----+------+------+-------------+
+----+------+------+-------------+

# Last but not least, you'll also want to define endpoints in Keystone
# (2)Import endpoints to keystone
wget http://www.hastexo.com/system/files/user/4/endpoints.sh__0.txt
mv endpoints.sh__0.txt endpoints.sh
mod +x endpoints.sh

./endpoints.sh -h
#Usage: ./endpoints.sh [-m mysql_hostname] [-u mysql_username] [-D mysql_database] [-p mysql_password]
#       [-K keystone_master ] [ -R keystone_region ] [ -E keystone_endpoint_url ] 
#       [ -S swift_master ] [ -T keystone_token ]

./endpoints.sh -m 192.168.1.200 -u keystonedbadmin -D keystone -p keystonedbpass -K 192.168.1.200 -R RegionOne -E "http://localhost:35357/v2.0" -S 192.168.1.200 -T KEZUNLIN

# or
vim endpoints.sh
#**************************************************
#**************************************************
MYSQL_HOST=192.168.1.200
MYSQL_DATABASE=keystone
MYSQL_USER=keystonedbadmin
MYSQL_PASSWORD=keystonedbpass
MASTER=192.168.1.200
KEYSTONE_REGION=RegionOne
export SERVICE_ENDPOINT="http://localhost:35357/v2.0" 
SWIFT_MASTER=192.168.1.200
export SERVICE_TOKEN=KEZUNLIN
#**************************************************
#**************************************************
./endpoints.sh

# select from keystone db
mysql -u root -prootpass <<EOF 
use keystone;
select id,type,extra from service;
EOF
+----------------------------------+--------------+---------------------------------------------------------------+
| id                               | type         | extra                                                         |
+----------------------------------+--------------+---------------------------------------------------------------+
| 3ebd8e5607ed4bdf9779f914e4b77fcc | ec2          | {"description": "OpenStack EC2 service", "name": "ec2"}       |
| 43396ca3bdd045e38433cc000bb686ac | identity     | {"description": "OpenStack Identity", "name": "keystone"}     |
| 524daea51ebf4a159d86388bba56ed74 | compute      | {"description": "OpenStack Compute Service", "name": "nova"}  |
| 9228fd00d75e4856a675f2d9e01292b6 | image        | {"description": "OpenStack Image Service", "name": "glance"}  |
| bd9ed782d2aa45c3835fa816d39afd2f | object-store | {"description": "OpenStack Storage Service", "name": "swift"} |
| e0918fbff4004e6e97d38e599554a345 | volume       | {"description": "OpenStack Volume Service", "name": "volume"} |
+----------------------------------+--------------+---------------------------------------------------------------+

# test keystone
keystone service-list
+----------------------------------+----------+--------------+---------------------------+
|                id                |   name   |     type     |        description        |
+----------------------------------+----------+--------------+---------------------------+
| 3ebd8e5607ed4bdf9779f914e4b77fcc | ec2      | ec2          | OpenStack EC2 service     |
| 43396ca3bdd045e38433cc000bb686ac | keystone | identity     | OpenStack Identity        |
| 524daea51ebf4a159d86388bba56ed74 | nova     | compute      | OpenStack Compute Service |
| 9228fd00d75e4856a675f2d9e01292b6 | glance   | image        | OpenStack Image Service   |
| bd9ed782d2aa45c3835fa816d39afd2f | swift    | object-store | OpenStack Storage Service |
| e0918fbff4004e6e97d38e599554a345 | volume   | volume       | OpenStack Volume Service  |
+----------------------------------+----------+--------------+---------------------------+

 keystone endpoint-list
+----------------------------------+-----------+-------------------------------------------------+-------------------------------------------------+--------------------------------------------+
|                id                |   region  |                    publicurl                    |                   internalurl                   |                  adminurl                  |
+----------------------------------+-----------+-------------------------------------------------+-------------------------------------------------+--------------------------------------------+
| 3bb81f2b12c84815bf262847a12a97e1 | RegionOne | http://192.168.1.200:5000/v2.0                  | http://192.168.1.200:5000/v2.0                  | http://192.168.1.200:35357/v2.0            |
| 6208b5029eb54c86ab311887b5bb1170 | RegionOne | http://192.168.1.200:8774/v2/%(tenant_id)s      | http://192.168.1.200:8774/v2/%(tenant_id)s      | http://192.168.1.200:8774/v2/%(tenant_id)s |
| 6eda24daf26c43439e37e37ea03057c9 | RegionOne | http://192.168.1.200:8773/services/Cloud        | http://192.168.1.200:8773/services/Cloud        | http://192.168.1.200:8773/services/Admin   |
| 83822f48404745d4842e82b3e9be900d | RegionOne | http://192.168.1.200:8776/v1/%(tenant_id)s      | http://192.168.1.200:8776/v1/%(tenant_id)s      | http://192.168.1.200:8776/v1/%(tenant_id)s |
| 8f6f2c91a1684ef0a9ed5a1b61bfce18 | RegionOne | http://192.168.1.200:8080/v1/AUTH_%(tenant_id)s | http://192.168.1.200:8080/v1/AUTH_%(tenant_id)s | http://192.168.1.200:8080/v1               |
| b84ff72497874100bd8e113b524c88d1 | RegionOne | http://192.168.1.200:9292/v1                    | http://192.168.1.200:9292/v1                    | http://192.168.1.200:9292/v1               |
+----------------------------------+-----------+-------------------------------------------------+-------------------------------------------------+--------------------------------------------+


keystone token-get
+-----------+----------------------------------+
|  Property |              Value               |
+-----------+----------------------------------+
| expires   | 2013-11-22T04:23:49Z             |
| id        | b075e6202055458b9d3538cb2d5ebd94 |
| tenant_id | fe949fccd7cb48629b67ae074a430a57 |
| user_id   | 5cea87080d1d4f02b2587489c0c0dd95 |
+-----------+----------------------------------+

# how to test keystone with curl
curl -d '{"auth": {"tenantName": "admin", "passwordCredentials":{"username": "admin", "password": "kezunlin"}}}' -H "Content-type: application/json" http://127.0.0.1:35357/v2.0/tokens | python -mjson.tool

#====================================================
#Step 4: Install and configure Glance
#====================================================
apt-get -y install glance glance-api glance-client glance-common glance-registry python-glance
vim /etc/glance/glance-api-paste.ini 
# use variables defined in keystone_data.sh
#**************************************************
#**************************************************
#admin_tenant_name = %SERVICE_TENANT_NAME% 
#admin_user = %SERVICE_USER% 
#admin_password = %SERVICE_PASSWORD%
admin_tenant_name = service
admin_user = glance
admin_password = kezunlin
#**************************************************
#**************************************************

vim /etc/glance/glance-registry-paste.ini 
#**************************************************
#**************************************************
#admin_tenant_name = %SERVICE_TENANT_NAME% 
#admin_user = %SERVICE_USER% 
#admin_password = %SERVICE_PASSWORD%
admin_tenant_name = service
admin_user = glance
admin_password = kezunlin
#**************************************************
#**************************************************

vim /etc/glance/glance-registry.conf
#**************************************************
#**************************************************
#sql_connection = sqlite:////var/lib/glance/glance.sqlite
sql_connection = mysql://glancedbadmin:glancedbpass@192.168.1.200/glance
[paste_deploy]
flavor = keystone
#**************************************************
#**************************************************
#Notice: It's important to use the machine's actual IP in this example and not 127.0.0.1!  For example, my ip is 192.168.1.200
#[paste_deploy]
#flavor = keystone
# These two lines instruct the Glance Registry to use Keystone for authentication, which is what we want.

vim /etc/glance/glance-api.conf
#**************************************************
#**************************************************
[paste_deploy]
flavor = keystone
#**************************************************
#**************************************************

# Afterwards, you need to initially synchronize the Glance database by running these commands:
glance-manage version_control 0
glance-manage db_sync         
# /usr/lib/python2.7/dist-packages/glance/registry/db/migrate_repo/versions/003_add_disk_format.py:47: SADeprecationWarning: useexisting is deprecated.  Use extend_existing.useexisting=True)

#It's time to restart Glance now:
service glance-api restart && service glance-registry restart

# Now what's the best method to verify that Glance is working as expected? The glance command line utilty can do that for us, but to work properly, it needs to know how we want to authenticate ourselves to Glance (and keystone, subsequently). This is a very good moment to define four environmental variables that we'll need continously when working with OpenStack: OS_TENANT_NAME, OS_USERNAME, OS_PASSWORD and OS_AUTH_URL.  

# verify that Glance is working as expected
# make sure OS_ has been exported
source novarc
env | grep OS_
#OS_PASSWORD=kezunlin
#OS_AUTH_URL=http://localhost:5000/v2.0/
#OS_USERNAME=admin
#OS_TENANT_NAME=admin

# After exporting these variables, you should be able to do:
glance index
echo $?
#0
# get no output at all in return (but the return code will be 0; check with echo $?). If that's the case, Glance is setup correctly and properly connects with Keystone. 

glance --version
#glance 2012.1.3-dev

# Now let's add our first image! 

# Upload images
# disk format :aki ami ari  iso raw vmdk
# container format: aki ami ari ovf bare
# Mapping disk_format-container_format
# raw-bare vmdk-bare iso-bare aki-aki ari-ari ami-ami

#==================================
# Test 1): download and test tty 
#==================================
mkdir stackimages
wget -c http://images.ansolabs.com/tty.tgz -O stackimages/tty.tgz
tar -xzvf stackimages/tty.tgz -C stackimages

glance add name="tty-kernel" is_public=true disk_format=aki container_format=aki < stackimages/aki-tty/image 
# kernel_id=e19d411a-69d8-4c61-8c57-06b2d064b914
glance add name="tty-ramdisk" is_public=true disk_format=ari container_format=ari < stackimages/ari-tty/image 
# ramdisk_id=da1209b9-8b99-4e37-a5fd-228b8b1e3c71
glance add name="tty" is_public=true disk_format=ami container_format=ami kernel_id=e19d411a-69d8-4c61-8c57-06b2d064b914 ramdisk_id=da1209b9-8b99-4e37-a5fd-228b8b1e3c71 < stackimages/ami-tty/image 
# id=3a4ae324-50a4-4fed-9ce5-7dd1fcb9f354

#==================================
# Test 2): download and test ubuntu 12.04 disk (220M)
# ubuntu 12.04 server amd64¿can be only login with ssh-key
#==================================
# http://cloud-images.ubuntu.com/
# http://cloud-images.ubuntu.com/precise/current/
# http://uec-images.ubuntu.com/releases/
# http://uec-images.ubuntu.com/releases/precise/release/
# download it from offical site, it may take a long time
wget http://192.168.1.184/openstack/ubuntu-12.04-server-cloudimg-amd64-disk1.img

# Then add this image to Glance:
glance add name="Ubuntu 12.04 server cloudimg amd64" is_public=true disk_format=qcow2 container_format=ovf  < ./image/ubuntu-12.04-server-cloudimg-amd64-disk1.img
#Added new image with ID: a3f0131a-8a44-4d08-ad52-f54634ddeaa3

#==================================
# Test 3): download and test cirros (10M)
# Cirros¿can be login with username and password or with ssh-key
#user:cirros
#password:cubswin:)
#==================================
#wget https://launchpad.net/cirros/trunk/0.3.0/+download/cirros-0.3.0-x86_64-disk.img
wget http://192.168.1.184/openstack/cirros-0.3.0-x86_64-disk.img

# Then add this image to Glance:
glance add name="cirros-0.30-x86_64" is_public=true disk_format=qcow2 container_format=bare  < ./image/cirros-0.3.0-x86_64-disk.img
# Added new image with ID: 43e71758-a376-409c-8fb7-7050f89ddb6a

# After this, you should be seeing the freshly added image.
glance index
#ID                                   Name                           Disk Format          Container Format     Size          
#------------------------------------ ------------------------------ -------------------- -------------------- --------------
#43e71758-a376-409c-8fb7-7050f89ddb6a cirros-0.30-x86_64             qcow2                bare                        9761280
#a3f0131a-8a44-4d08-ad52-f54634ddeaa3 Ubuntu 12.04 server cloudimg a qcow2                ovf                       254738432

#====================================================
# Step 5: Install and configure Nova
#====================================================
apt-get -y install nova-api nova-cert nova-common nova-compute nova-compute-kvm nova-doc nova-network nova-objectstore nova-scheduler nova-volume nova-consoleauth novnc python-nova python-novaclient 

chown -R nova:nova /etc/nova

# if you are installing openstack in a virtual machine,change --libvirt_type from kvm to qemu.
vim /etc/nova/nova.conf
#**************************************************
#**************************************************
# ...
#**************************************************
#**************************************************

vim /etc/nova/nova-compute.conf
#**************************************************
#**************************************************
#--libvirt_type=kvm
--libvirt_type=qemu
#**************************************************
#**************************************************

vim /etc/nova/api-paste.ini
#**************************************************
#**************************************************
#admin_tenant_name = %SERVICE_TENANT_NAME% 
#admin_user = %SERVICE_USER% 
#admin_password = %SERVICE_PASSWORD%
admin_tenant_name = service
admin_user = nova
admin_password = kezunlin
#**************************************************
#**************************************************

# Then, restart all nova services to make the configuration file changes take effect:
vim restart_nova.sh
#**************************************************
#**************************************************
#!/bin/bash
for a in libvirt-bin nova-network nova-compute nova-cert nova-api nova-objectstore nova-scheduler nova-volume novnc nova-consoleauth; do service "$a" stop; done

for a in libvirt-bin nova-network nova-compute nova-cert nova-api nova-objectstore nova-scheduler nova-volume novnc nova-consoleauth; do service "$a" start; done
#**************************************************
#**************************************************

chmod +x restart_nova.sh 
bash restart_nova.sh

# The next step will create all tables Nova needs in MySQL. While we are at it, we can also create the network we want to use for our VMs in the Nova databases. Do this:
nova-manage db sync
# 2013-11-20 21:45:26 DEBUG nova.utils [-] backend <module 'nova.db.sqlalchemy.migration' from '/usr/lib/python2.7/dist-packages/nova/db/sqlalchemy/migration.pyc'> from (pid=27219) __get_backend /usr/lib/python2.7/dist-packages/nova/utils.py:663
#2013-11-20 21:45:31 WARNING nova.utils [-] /usr/lib/python2.7/dist-packages/sqlalchemy/pool.py:639: SADeprecationWarning: The 'listeners' argument to Pool (and create_engine()) is deprecated.  Use event.listen().
#  Pool.__init__(self, creator, **kw)
#
#2013-11-20 21:45:31 WARNING nova.utils [-] /usr/lib/python2.7/dist-packages/sqlalchemy/pool.py:145: SADeprecationWarning: Pool.add_listener is deprecated.  Use event.listen()
#  self.add_listener(l)
#
#2013-11-20 21:45:31 AUDIT nova.db.sqlalchemy.fix_dns_domains [-] Applying database fix for Essex dns_domains table.


# select from nova db
mysql -u root -prootpass <<EOF 
use nova;
show tables;
EOF
#Tables_in_nova
#agent_builds
#aggregate_hosts
#aggregate_metadata
#aggregates
#auth_tokens
#block_device_mapping
#bw_usage_cache
#cells
#certificates
#compute_nodes
#console_pools
#consoles
#dns_domains
#fixed_ips
#floating_ips
#instance_actions
#instance_faults
#instance_info_caches
#instance_metadata
#instance_type_extra_specs
#instance_types
#instances
#iscsi_targets
#key_pairs
#migrate_version
#migrations
#networks
#projects
#provider_fw_rules
#quotas
#s3_images
#security_group_instance_association
#security_group_rules
#security_groups
#services
#sm_backend_config
#sm_flavors
#sm_volume
#snapshots
#user_project_association
#user_project_role_association
#user_role_association
#users
#virtual_interfaces
#virtual_storage_arrays
#volume_metadata
#volume_type_extra_specs
#volume_types
#volumes

nova-manage network create private --fixed_range_v4=192.168.2.32/27 --num_networks=1 --bridge=br100 --bridge_interface=eth0 --network_size=32 
# create 1 record in table 'networks'
# create 32 records in table 'fixed_ips'

mysql -u root -prootpass nova <<EOF
select id,cidr from networks;
EOF
#id	cidr
#1	192.168.2.32/27

mysql -u root -prootpass nova <<EOF
select id,address,network_id from fixed_ips;
EOF
#id	address	network_id
#1	192.168.2.32	1
#2	192.168.2.33	1
#3	192.168.2.34	1
#4	192.168.2.35	1
#5	192.168.2.36	1
#6	192.168.2.37	1
#7	192.168.2.38	1
#8	192.168.2.39	1
#9	192.168.2.40	1
#10	192.168.2.41	1
#11	192.168.2.42	1
#12	192.168.2.43	1
#13	192.168.2.44	1
#14	192.168.2.45	1
#15	192.168.2.46	1
#16	192.168.2.47	1
#17	192.168.2.48	1
#18	192.168.2.49	1
#19	192.168.2.50	1
#20	192.168.2.51	1
#21	192.168.2.52	1
#22	192.168.2.53	1
#23	192.168.2.54	1
#24	192.168.2.55	1
#25	192.168.2.56	1
#26	192.168.2.57	1
#27	192.168.2.58	1
#28	192.168.2.59	1
#29	192.168.2.60	1
#30	192.168.2.61	1
#31	192.168.2.62	1
#32	192.168.2.63	1

# restart nova again
bash restart_nova.sh

# list all nova-* processes
ps -ef | grep nova

nova-manage service list
#Binary           Host                                 Zone             Status     State Updated_At
#nova-consoleauth master                               nova             enabled    :-)   2013-11-21 09:49:00
#nova-cert        master                               nova             enabled    :-)   2013-11-21 09:49:00
#nova-scheduler   master                               nova             enabled    :-)   2013-11-21 09:49:00
#nova-compute     master                               nova             enabled    :-)   2013-11-21 09:49:02
#nova-network     master                               nova             enabled    :-)   2013-11-21 09:49:01

#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
# nova-compute failed to start
# http://blog.csdn.net/hilyoo/article/details/7746634
# solution:
ps -ef | grep dmidecode
killall -9 dmidecode
# when use virsh, dmidecode will be started
#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

source novarc
nova help

# list all currently running VMs (none, the list should be empty
nova list

# list all images uploaded to glance
nova image-list
+--------------------------------------+------------------------------------+--------+--------+
|                  ID                  |                Name                | Status | Server |
+--------------------------------------+------------------------------------+--------+--------+
| 43e71758-a376-409c-8fb7-7050f89ddb6a | cirros-0.30-x86_64                 | ACTIVE |        |
| a3f0131a-8a44-4d08-ad52-f54634ddeaa3 | Ubuntu 12.04 server cloudimg amd64 | ACTIVE |        |
+--------------------------------------+------------------------------------+--------+--------+


#====================================================
# Step 6: Your first VM
#====================================================
# Once Nova works as desired, starting your first own cloud VM is easy

ssh-keygen
#Generating public/private rsa key pair.
#Enter file in which to save the key (/root/.ssh/id_rsa): 
#Enter passphrase (empty for no passphrase): 
#Enter same passphrase again: 
#Your identification has been saved in /root/.ssh/id_rsa.
#Your public key has been saved in /root/.ssh/id_rsa.pub.
#The key fingerprint is:
#84:e0:6f:12:6b:51:2d:81:d0:15:59:2b:c2:cb:7f:0d root@ubuntu

nova keypair-add --pub_key /root/.ssh/id_rsa.pub key1
# nova keypair-delete keypairName
#nova keypair-delete key1

#This will add the key to OpenStack Nova and store it with the name "key1". The only thing left to do after this is firing up your VM. 

nova keypair-list
+------+-------------------------------------------------+
| Name |                   Fingerprint                   |
+------+-------------------------------------------------+
| key1 | 98:1f:be:5c:33:38:4d:d5:60:7c:b9:7c:9d:fc:c3:f8 |
+------+-------------------------------------------------+

#Find out what ID your Ubuntu image has, you can do this with:
nova image-list
+--------------------------------------+------------------------------------+--------+--------+
|                  ID                  |                Name                | Status | Server |
+--------------------------------------+------------------------------------+--------+--------+
| 43e71758-a376-409c-8fb7-7050f89ddb6a | cirros-0.30-x86_64                 | ACTIVE |        |
| a3f0131a-8a44-4d08-ad52-f54634ddeaa3 | Ubuntu 12.04 server cloudimg amd64 | ACTIVE |        |
+--------------------------------------+------------------------------------+--------+--------+

# When starting a VM, you also need to define the flavor it is supposed to use. Flavors are pre-defined hardware schemes in OpenStack with which you can define what resources your newly created VM has. OpenStack comes with five pre-defined flavors; you can get an overview over the existing flavors with
nova flavor-list
+----+-----------+-----------+------+-----------+------+-------+-------------+
| ID |    Name   | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor |
+----+-----------+-----------+------+-----------+------+-------+-------------+
| 1  | m1.tiny   | 512       | 0    | 0         |      | 1     | 1.0         |
| 2  | m1.small  | 2048      | 10   | 20        |      | 1     | 1.0         |
| 3  | m1.medium | 4096      | 10   | 40        |      | 2     | 1.0         |
| 4  | m1.large  | 8192      | 10   | 80        |      | 4     | 1.0         |
| 5  | m1.xlarge | 16384     | 10   | 160       |      | 8     | 1.0         |
+----+-----------+-----------+------+-----------+------+-------+-------------+

#Flavors are referenced by their ID, not by  their name. That's important for the actual command to execute to start your VM. That command's syntax basically is this:
#nova boot --flavor ID --image Image-UUID --key_name key-name vm_name

# let us use cirros, its image id is 43e71758-a376-409c-8fb7-7050f89ddb6a
nova boot --flavor 1 --image 43e71758-a376-409c-8fb7-7050f89ddb6a --key_name key1 vm1 
nova boot --flavor 1 --image 43e71758-a376-409c-8fb7-7050f89ddb6a --key_name key1 vm2

#nova delete vmID
#nova delete 14c9285d-7709-4594-9f53-df0eb7c5a274  

nova show vm1
# id=999292bc-a25e-4493-b9ad-0ca864049afe 
# status=BUILD---->ACTIVE
# private network =192.168.2.33 

# nova pause | unpause | resume | reboot | boot | resize |
nova list
+--------------------------------------+------+--------+----------------------+
|                  ID                  | Name | Status |       Networks       |
+--------------------------------------+------+--------+----------------------+
| f20bd401-9202-4250-bed4-c051a1379b0d | vm1  | ACTIVE | private=192.168.2.34 |
+--------------------------------------+------+--------+----------------------+

# Now it's time to access our first vm.
#user:cirros
#password:cubswin:)
ssh cirros@192.168.2.34
# OR
ssh -i /root/.ssh/id_rsa cirros@192.168.2.34

root@master:~/ke/guides/openstack# ssh cirros@192.168.2.34
#$ uname -a
#Linux cirros 3.0.0-12-virtual #20-Ubuntu SMP Fri Oct 7 18:19:02 UTC 2011 x86_64 GNU/Linux
#$ ifconfig
#eth0      Link encap:Ethernet  HWaddr FA:16:3E:75:11:29  
#          inet addr:192.168.2.34  Bcast:192.168.2.63  Mask:255.255.255.224
#          inet6 addr: fe80::f816:3eff:fe75:1129/64 Scope:Link
#          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
#          RX packets:7645 errors:0 dropped:0 overruns:0 frame:0
#          TX packets:298 errors:0 dropped:0 overruns:0 carrier:0
#          collisions:0 txqueuelen:1000 
#          RX bytes:498592 (486.9 KiB)  TX bytes:42651 (41.6 KiB)
#
#lo        Link encap:Local Loopback  
#          inet addr:127.0.0.1  Mask:255.0.0.0
#          inet6 addr: ::1/128 Scope:Host
#          UP LOOPBACK RUNNING  MTU:16436  Metric:1
#          RX packets:6 errors:0 dropped:0 overruns:0 frame:0
#          TX packets:6 errors:0 dropped:0 overruns:0 carrier:0
#          collisions:0 txqueuelen:0 
#          RX bytes:504 (504.0 B)  TX bytes:504 (504.0 B)
#
#$ exit
#Connection to 192.168.2.34 closed.

# scp file
scp 1.txt cirros@192.168.2.34:/home/cirros/

# Congratuations!!!
# Why?
ifconfig
route -n
#Kernel IP routing table
#Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
#0.0.0.0         192.168.1.1     0.0.0.0         UG    0      0        0 br100
#192.168.1.0     0.0.0.0         255.255.255.0   U     0      0        0 br100
#192.168.2.32    0.0.0.0         255.255.255.224 U     0      0        0 br100
#192.168.122.0   0.0.0.0         255.255.255.0   U     0      0        0 virbr0

ping 192.168.1.1
ping 192.168.2.33
ping 192.168.121.1

# ping vm
ping 192.168.2.34

# start vm2
nova boot --flavor 1 --image 43e71758-a376-409c-8fb7-7050f89ddb6a --key_name key1 vm2
nova list
+--------------------------------------+------+--------+----------------------+
|                  ID                  | Name | Status |       Networks       |
+--------------------------------------+------+--------+----------------------+
| 42334f70-fca6-4304-86e6-de26bc567e41 | vm2  | ACTIVE | private=192.168.2.35 |
| f20bd401-9202-4250-bed4-c051a1379b0d | vm1  | ACTIVE | private=192.168.2.34 |
+--------------------------------------+------+--------+----------------------+

ssh cirros@192.168.2.35
# OR
ssh -i /root/.ssh/id_rsa cirros@192.168.2.35
#root@master:~/ke/guides/openstack# ssh -i /root/.ssh/id_rsa cirros@192.168.2.35
#$ route -n
#Kernel IP routing table
#Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
#0.0.0.0         192.168.2.33    0.0.0.0         UG    0      0        0 eth0
#192.168.2.32    0.0.0.0         255.255.255.224 U     0      0        0 eth0
#$ ping 192.168.2.34
#PING 192.168.2.34 (192.168.2.34): 56 data bytes
#64 bytes from 192.168.2.34: seq=0 ttl=64 time=2.767 ms
#64 bytes from 192.168.2.34: seq=1 ttl=64 time=1.328 ms
#^C
#--- 192.168.2.34 ping statistics ---
#2 packets transmitted, 2 packets received, 0% packet loss
#round-trip min/avg/max = 1.328/2.047/2.767 ms

# ubuntu serve vm
nova boot --flavor 2 --image a3f0131a-8a44-4d08-ad52-f54634ddeaa3 --key_name key1 ubuntu1
nova list
ssh -i /root/.ssh/id_rsa ubuntu@192.168.2.36
#ubuntu@ubuntu1:~$ sudo passwd root
#Enter new UNIX password: 
#Retype new UNIX password: 
#passwd: password updated successfully

#==============================================================
# view nova db
mysql -u root
use nova;
select id,address,instance_id from fixed_ips;
select id,address,fixed_ip_id from floating_ips;

select id from instances;
select id,instance_id,network_info from instance_info_caches;
describe security_group_instance_association;
select id,security_group_id,instance_id from security_group_instance_association;
select id,name from security_groups;

# how to delete instances
SET FOREIGN_KEY_CHECKS=0;
delete from instances where id = '29';
delete from instances where id = '30';
SET FOREIGN_KEY_CHECKS=1;
#==============================================================

#====================================================
# Step 7: The OpenStack Dashboard
#====================================================
# We can use Nova to start and stop virtual machines now, but up to this point, we can only do it on the command line. That's not good, because typically, we'll want users without high-level administrator skills to be able to start new VMs. There's a solution for this on the OpenStack ecosystem called Dashboard, codename Horizon. Horizon is OpenStack's main configuration interface. It's django-based.

# Dashboard
apt-get -y install apache2 libapache2-mod-wsgi openstack-dashboard

vim /etc/openstack-dashboard/local_settings.py 
#**************************************************
#**************************************************
#CACHE_BACKEND = 'locmem://'
CACHE_BACKEND = 'memcached://127.0.0.1:11211/'
#**************************************************
#**************************************************

service apache2 restart
# After this, point your webbrowser to the Nova machine's IP address and you should see the OpenStack Dashboard login prompt. Login with admin and the password you specified. That's it - you're in!

http:192.168.1.200:880
# (admin,kezunlin)

#====================================================
# Step 8: Using floating IPs
#====================================================
# Floating IPs are an unbelievably handy tool in OpenStack to supply your virtual machines with "official" IP addresses. In this example, we've mainly been dealing with the 192.168.22.0/24 network, which is the "internal" network for our VMs. Our VMs can communicate with each other and they can communicate with the outside world, but they don't have an official IP address that others could connect to (the "public" net in this test-setup is 10.42.0.0/24 after all). Floating IPs allow you to assign your VMs an additional IP from that "public" network, making them accessible directly. And using floating IPs is anything but hard!

# First, you'll have to define a range of addresses which OpenStack nova will use. Our old friend nova-manage does this: 
nova-manage floating create --ip_range=192.168.1.32/27
# create records in nova.floating_ips

mysql -u root -prootpass nova 
select id,address from floating_ips;
#+----+--------------+
#| id | address      |
#+----+--------------+
#|  1 | 192.168.1.33 |
#|  2 | 192.168.1.34 |
#|  3 | 192.168.1.35 |
#|  4 | 192.168.1.36 |
#|  5 | 192.168.1.37 |
#|  6 | 192.168.1.38 |
#|  7 | 192.168.1.39 |
#|  8 | 192.168.1.40 |
#|  9 | 192.168.1.41 |
#| 10 | 192.168.1.42 |
#| 11 | 192.168.1.43 |
#| 12 | 192.168.1.44 |
#| 13 | 192.168.1.45 |
#| 14 | 192.168.1.46 |
#| 15 | 192.168.1.47 |
#| 16 | 192.168.1.48 |
#| 17 | 192.168.1.49 |
#| 18 | 192.168.1.50 |
#| 19 | 192.168.1.51 |
#| 20 | 192.168.1.52 |
#| 21 | 192.168.1.53 |
#| 22 | 192.168.1.54 |
#| 23 | 192.168.1.55 |
#| 24 | 192.168.1.56 |
#| 25 | 192.168.1.57 |
#| 26 | 192.168.1.58 |
#| 27 | 192.168.1.59 |
#| 28 | 192.168.1.60 |
#| 29 | 192.168.1.61 |
#| 30 | 192.168.1.62 |
#+----+--------------+
#30 rows in set (0.00 sec)

# Then, within Nova itself, you'll have to create a floating IP (creating here is Nova-speak for "reserving"):
nova floating-ip-create
# nova floating-ip-delete 192.168.1.33
+---------------+-------------+----------+------+
|       Ip      | Instance Id | Fixed Ip | Pool |
+---------------+-------------+----------+------+
| 192.168.1.33 | None        | None     | nova |
+---------------+-------------+----------+------+

# To assign this IP to our vm1, use this command: 
nova add-floating-ip vm1 192.168.1.33
# private network  192.168.2.34, 192.168.1.33

# delete floating ip
#nova remove-floating-ip vm1 192.168.1.33

# Please note: Assigning a floating IP to an existing VM does automatically enable that IP for the VM. You'll not have to manually assign the IP to the VMs main network interface, as all the networking magic is done by iptables on the actual compute node.
#That's it! Your new VM can now use its floating IP. There is only one problem left: By default, nova uses very secure iptables rules to protect IPs reachable via floating IPs from abuse. De facto, nova will not allow any traffic from the outside to get through to your VM. We'll have to fiddle with Security Groups to solve this problem. Here's how you can enable SSH access and ICMP to your floating IPs:

# modify security group rule so that we can visit the vm1 by floating IP.
nova secgroup-add-rule default tcp 22 22 0.0.0.0/0
+-------------+-----------+---------+-----------+--------------+
| IP Protocol | From Port | To Port |  IP Range | Source Group |
+-------------+-----------+---------+-----------+--------------+
| tcp         | 22        | 22      | 0.0.0.0/0 |              |
+-------------+-----------+---------+-----------+--------------+

nova secgroup-add-rule default icmp -1 -1 0.0.0.0/0
+-------------+-----------+---------+-----------+--------------+
| IP Protocol | From Port | To Port |  IP Range | Source Group |
+-------------+-----------+---------+-----------+--------------+
| icmp        | -1        | -1      | 0.0.0.0/0 |              |
+-------------+-----------+---------+-----------+--------------+

nova secgroup-list
+---------+-------------+
|   Name  | Description |
+---------+-------------+
| default | default     |
+---------+-------------+

nova secgroup-list-rules default
+-------------+-----------+---------+-----------+--------------+
| IP Protocol | From Port | To Port |  IP Range | Source Group |
+-------------+-----------+---------+-----------+--------------+
| icmp        | -1        | -1      | 0.0.0.0/0 |              |
| tcp         | 22        | 22      | 0.0.0.0/0 |              |
+-------------+-----------+---------+-----------+--------------+

# After this, your VM will be reachable directly from the outside via its floating IP address (by SSH and ICMP).

# Test
# Now go to another machine and ssh to vm1
# 1)user,password login mode
root@slave:~# ssh cirros@192.168.1.33
# input password cubswin:)
# 2)ssh-key login mode
root@slave:~# scp master:/root/.ssh/id_rsa id_rsa_33
root@slave:~# ssh -i id_rsa_33 cirros@192.168.1.33

# or ping vm1
root@slave:~# ping cirros@192.168.1.33
# OK

# vnc to vm
nova get-vnc-console vm1 novnc
+-------+------------------------------------------------------------------------------------+
|  Type |                                        Url                                         |
+-------+------------------------------------------------------------------------------------+
| novnc | http://192.168.1.200:6080/vnc_auto.html?token=9ead53c3-1d1b-4233-9963-37834bbdc6e3 |
+-------+------------------------------------------------------------------------------------+

# open your chrome, access http://192.168.1.200:6080/vnc_auto.html?token=9ead53c3-1d1b-4233-9963-37834bbdc6e3
# OK
# cirros,cubswin:)

#====================================================
# Appendix: Making the euca2ools work
#====================================================
# OpenStack offers a full-blown native API for administrator interaction. However, it also has an API compatible with Amazons AWS service. This means that on Linux you can not only use the native OpenStack clients for interaction but also the euca2ools toolsuite.
apt-get install -y euca2ools

# First, make sure that the $SERVICE_TOKEN and $SERVICE_ENDPOINT variables are not set:
unset SERVICE_TOKEN
unset SERVICE_ENDPOINT
echo $SERVICE_TOKEN
echo $SERVICE_ENDPOINT

# Then, make sure the numerous OS_* variables are set correctly 
souce novarc
env | grep OS_

# Now you can go ahead:
export EC2_URL=$(keystone catalog --service ec2 | awk '/ publicURL / { print $4 }')
export CREDS=$(keystone ec2-credentials-create)
export EC2_ACCESS_KEY=$(echo "$CREDS" | awk '/ access / { print $4 }')
export EC2_SECRET_KEY=$(echo "$CREDS" | awk '/ secret / { print $4 }')

env | grep EC2_
#EC2_SECRET_KEY=9cf52e4f2a6749c88408283747e717d7
#EC2_URL=http://192.168.1.200:8773/services/Cloud
#EC2_ACCESS_KEY=536ed570ceb941e59b30773aa8df4a2e

#After that, the euca2ools should just work (if the euca2ools package is installed, of course). You can try running
euca-describe-images
#IMAGE	ami-00000002	None (Ubuntu 12.04 server cloudimg amd64)		available	public			machine				instance-store
#IMAGE	ami-00000001	None (cirros-0.30-x86_64)		available	public			machine				instance-store

# or
euca-describe-instances

euca-describe-availability-zones verboese

euca-add-keypair mykey >mykey.priv
# euca-run-instances -k mykey -t m1.tiny $AMI_ID
# $AMI_ID comes from euca-describe-images
euca-describe-images 
# aki-00000001 ari-00000002 ami-00000003
euca-run-instances -k mykey -t m1.tiny ami-00000003
# instance  $VALUE=i-00000001   $IP_ADDR=10.0.0.3
euca-describe-instances

# ssh -i mykey.priv $USER@$IP_ADDR
# $USER=ubuntu
# $IP_ADDR(euca-describe-instances)
ssh -i mykey.priv ubuntu@10.0.0.3
# how to terminate the instance when you are done with it
# euca-terminate-instances $VALUE
euca-terminate-instances i-00000001

euca-allocate-address
euca-release-address
euca-release-address 192.168.1.194
euca-associate-address -i i-0000003 192.168.1.194
euca-disassociate-address  192.168.1.194
euca-describe-groups

#====================================================
# Appendix: Making nova-volume work
#====================================================
# nova-volume is the OpenStack Compute component that will allow you to assign persistent storage devices to your virtual machines. Internally, it's using iSCSI, which is why you installed the tgt package earlier.

# create a new file to act as LVM.
dd if=/dev/zero of=/opt/nova-volumes.img bs=1M seek=100000 count=0
losetup -f nova-volumes.img
losetup -a
/dev/loop0: [0801]:35127298 (/opt/nova-volumes.img)
vgcreate nova-volumes /dev/loop0
#  No physical volume label read from /dev/loop0
#  Physical volume "/dev/loop0" successfully created
#  Volume group "nova-volumes" successfully created

vgdisplay

# create a 1G volume named volume1
nova volume-create --display_name "volume1" 1
nova volume-list
+----+----------+--------------+------+-------------+-------------+
| ID |  Status  | Display Name | Size | Volume Type | Attached to |
+----+----------+--------------+------+-------------+-------------+
| 1  | creating | volume1      | 1    | None        |             |
+----+----------+--------------+------+-------------+-------------+
nova volume-attach VM_ubuntu 1 /dev/vdb

# how to delete volume
euca-describe-volumes
# vol-00000001
euca-delete-volume vol-00000001


#========================================================================
#************************************************************************
# Problems:
#************************************************************************
#========================================================================
# http://serverfault.com/questions/460348/why-cant-openstack-vm-reach-itself-via-its-floating-ip

