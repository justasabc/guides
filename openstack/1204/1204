# A quick guide to install openstack essex on ubuntu 12.04
#----------------------------------------------------------------------
# http://www.hastexo.com/resources/docs/installing-openstack-grizzly-20131-ubuntu-1204-precise-pangolin
# http://www.chenshake.com/ubuntu12-04-2-installed-openstack-grizzly-bridge-mode/
# http://www.mirantis.com/blog/openstack-networking-flatmanager-and-flatdhcpmanager/
#----------------------------------------------------------------------
# 1204

#====================================================
# Step 1: Prepare your System
#====================================================
apt-get update
apt-get -y upgrade
apt-get -y install bridge-utils

vim /etc/network/interfaces
#**************************************************
auto lo
iface lo inet loopback

auto eth0
iface eth0 inet manual
	pre-up ifconfig eth0 up
	post-down ifconfig eth0 down

auto br100
iface br100 inet static
        address 192.168.1.200
        netmask 255.255.255.0
        network 192.168.1.0
        broadcast 192.168.1.255
        gateway 192.168.1.1
	dns-search example.com test.example.com
	dns-nameservers 192.168.1.1

        bridge_ports eth0
        bridge_fd 9
        bridge_hello 2
        bridge_maxage 12
        bridge_stp off
#**************************************************
/etc/init.d/networking restart

# install ntp and configure
apt-get -y install ntp
vim /etc/ntp.conf
server ntp.ubuntu.com iburst
server 127.127.1.0
fudge 127.127.1.0 stratum 10

service ntp restart
/etc/init.d/ntp restart
 
# install tgt target (we'll need it for nova-volume) 
apt-get -y install tgt
service tgt restart

# install iscsi client
apt-get -y install open-iscsi open-iscsi-utils

# We'll also need RabbitMQ, an AMQP-implementation, as that is what all OpenStack components use to communicate with eath other, and memcached.
apt-get -y install rabbitmq-server memcached python-memcache

# As we'll also want to run KVM virtual machines on this very same host, we'll need KVM and libvirt, which OpenStack uses to control virtual machines.
apt-get -y install kvm libvirt-bin

#====================================================
#Step 2: Install MySQL and create the necessary databases and users
#====================================================
# Nova and glance will use MySQL to store their runtime data. To make sure they can do that, we'll install and set up MySQL
apt-get install -y mysql-server python-mysqldb
# Allow other machines to be able to talk to that MySQL database
sed -i 's/127.0.0.1/0.0.0.0/g' /etc/mysql/my.cnf  
service mysql restart

mysql -u root -prootpass <<EOF
CREATE DATABASE nova;
GRANT ALL PRIVILEGES ON nova.* TO 'novadbadmin'@'%' IDENTIFIED BY 'novadbpass' WITH GRANT OPTION;
CREATE DATABASE glance;
GRANT ALL PRIVILEGES ON glance.* TO 'glancedbadmin'@'%' IDENTIFIED BY 'glancedbpass' WITH GRANT OPTION;
EOF

# other
GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'rootpass' WITH GRANT OPTION;
SET PASSWORD FOR 'root'@'%' =PASSWORD('rootpass');
SET PASSWORD FOR 'root'@'localhost' =PASSWORD('rootpass');
SET PASSWORD FOR 'root'@'openstack' =PASSWORD('rootpass');
SET PASSWORD FOR 'root'@'127.0.0.1' =PASSWORD('rootpass');
SET PASSWORD FOR 'root'@'::1' =PASSWORD('rootpass');
quit;

apt-get -y install keystone python-keystone python-keystoneclient
vim /etc/keystone/keystone.conf
#admin_token= ADMIN
admin_token =KEZUNLIN

[catalog]
#driver = keystone.catalog.backends.sql.Catalog
driver = keystone.catalog.backends.templated.TemplatedCatalog
template_file = /etc/keystone/default_catalog.templates

service keystone restart

# Import data to keystone
wget http://www.hastexo.com/system/files/user/4/keystone_data.sh_.txt
mv keystone_data.sh_.txt keystone_data.sh
chmod +x keystone_data.sh

vim keystone_data.sh
#ADMIN_PASSWORD=${ADMIN_PASSWORD:-hastexo}
ADMIN_PASSWORD=${ADMIN_PASSWORD:-kezunlin}
SERVICE_PASSWORD=${SERVICE_PASSWORD:-$ADMIN_PASSWORD}
#export SERVICE_TOKEN="hastexo"
export SERVICE_TOKEN="KEZUNLIN"
export SERVICE_ENDPOINT="http://localhost:35357/v2.0"
SERVICE_TENANT_NAME=${SERVICE_TENANT_NAME:-service}

./keystone_data.sh
echo $?
0
# ./keystone_data.sh; echo $?
keystone --tenant=admin --username=admin --password=kezunlin  --auth_url=http://127.0.0.1:5000/v2.0 user-list
+----------------------------------+---------+--------------------+--------+
|                id                | enabled |       email        |  name  |
+----------------------------------+---------+--------------------+--------+
| 2b0d79fa05a74a2ca205294060e4fe18 | True    | nova@hastexo.com   | nova   |
| 7983720f688a41a1abf28d712c3b6f6e | True    | demo@hastexo.com   | demo   |
| 7b232a0fdefc46c4a1f127b094c8c1ff | True    | glance@hastexo.com | glance |
| f7f3454f91d542a9b691f1a24b1d8be8 | True    | admin@hastexo.com  | admin  |
+----------------------------------+---------+--------------------+--------+

keystone --tenant=admin --username=admin --password=kezunlin  --auth_url=http://127.0.0.1:5000/v2.0 tenant-list
+----------------------------------+--------------------+---------+
|                id                |        name        | enabled |
+----------------------------------+--------------------+---------+
| 022da686eebd410186e35c0b2c4b27de | invisible_to_admin | True    |
| 6d5a5185e9c247debc4fd10377a4d42f | admin              | True    |
| e59d6ee354ce4cbf8be1f4838160ef66 | demo               | True    |
| fec1001aa3c540b6908bbba260e7381d | service            | True    |
+----------------------------------+--------------------+---------+

keystone --tenant=admin --username=admin --password=kezunlin  --auth_url=http://127.0.0.1:5000/v2.0 role-list
+----------------------------------+----------------------+
|                id                |         name         |
+----------------------------------+----------------------+
| 40b0d25272b54196949a37af81da1842 | KeystoneAdmin        |
| 9bf30509ccb749ddb206e6072391099c | anotherrole          |
| c9676be8f8a2425f8556ff74c841fc20 | KeystoneServiceAdmin |
| f5e2f086f1e643c494e6e94455c90a96 | Member               |
| fef3f58086e447a2abff88490eba4e62 | admin                |
+----------------------------------+----------------------+

# how to test keystone with curl
curl -d '{"auth": {"tenantName": "admin", "passwordCredentials":{"username": "admin", "password": "kezunlin"}}}' -H "Content-type: application/json" http://127.0.0.1:35357/v2.0/tokens | python -mjson.tool


apt-get -y install glance glance-api glance-client glance-common glance-registry python-glance
vim /etc/glance/glance-api-paste.ini 
#admin_tenant_name = %SERVICE_TENANT_NAME% 
#admin_user = %SERVICE_USER% 
#admin_password = %SERVICE_PASSWORD%
admin_tenant_name = admin
admin_user = admin
admin_password = kezunlin

vim /etc/glance/glance-registry-paste.ini 
#admin_tenant_name = %SERVICE_TENANT_NAME% 
#admin_user = %SERVICE_USER% 
#admin_password = %SERVICE_PASSWORD%
admin_tenant_name = admin
admin_user = admin
admin_password = kezunlin

vim /etc/glance/glance-registry.conf
#sql_connection = sqlite:////var/lib/glance/glance.sqlite
sql_connection = mysql://glancedbadmin:glancedbpass@192.168.1.190/glance
[paste_deploy]
flavor = keystone

vim /etc/glance/glance-api.conf
[paste_deploy]
flavor = keystone

glance-manage version_control 0
glance-manage db_sync         
service glance-api restart && service glance-registry restart

export OS_TENANT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=kezunlin
export OS_AUTH_URL="http://localhost:5000/v2.0/"
# source novarc

env | grep OS_
OS_PASSWORD=kezunlin
OS_AUTH_URL=http://localhost:5000/v2.0/
OS_USERNAME=admin
OS_TENANT_NAME=admin

glance index
echo $?
0
glance --version
glance 2012.1

glance add name="tty-kernel" is_public=true disk_format=aki container_format=aki < stackimages/aki-tty/image 
# kernel_id=e19d411a-69d8-4c61-8c57-06b2d064b914
glance add name="tty-ramdisk" is_public=true disk_format=ari container_format=ari < stackimages/ari-tty/image 
# ramdisk_id=da1209b9-8b99-4e37-a5fd-228b8b1e3c71
glance add name="tty" is_public=true disk_format=ami container_format=ami kernel_id=e19d411a-69d8-4c61-8c57-06b2d064b914 ramdisk_id=da1209b9-8b99-4e37-a5fd-228b8b1e3c71 < stackimages/ami-tty/image 
# id=3a4ae324-50a4-4fed-9ce5-7dd1fcb9f354


# download the image by using xunlei and scp to the destionation
wget http://uec-images.ubuntu.com/releases/11.10/release/ubuntu-11.10-server-cloudimg-amd64-disk1.img
scp ke@192.168.1.170:/mnt/hgfs/SharedFolder/download/ubuntu-11.10-server-cloudimg-amd64-disk1.img .

glance add name="Ubuntu 11.10 cloudimg amd64" is_public=true disk_format=qcow2 container_format=ovf  < /root/ubuntu-11.10-server-cloudimg-amd64-disk1.img 
# id=efdec9aa-704c-45be-8d16-5c2bf20a4c63

glance add name="Ubuntu 11.10 desktop i386 iso" is_public=true disk_format=iso container_format=bare  < /root/ubuntu11.iso 
# id=afe410a8-1ba2-45d8-a69f-a1ee6fec8a77

glance add name="Ubuntu 11.10 desktop i386 vmdk" is_public=true disk_format=vmdk container_format=bare  < /root/ubuntu11.vmdk 
# id=27e18271-fda5-4b95-b44a-a42f31bb344f

glance index

apt-get -y install nova-api nova-cert nova-common nova-compute nova-compute-kvm nova-doc nova-network nova-objectstore nova-scheduler nova-vncproxy nova-volume python-nova python-novaclient

vim /etc/nova/nova.conf
# ...

vim /etc/nova/api-paste.ini
#admin_tenant_name = %SERVICE_TENANT_NAME% 
#admin_user = %SERVICE_USER% 
#admin_password = %SERVICE_PASSWORD%
admin_tenant_name = admin
admin_user = admin
admin_password = kezunlin

for a in libvirt-bin nova-network nova-compute nova-api nova-objectstore nova-scheduler nova-volume nova-vncproxy; do service "$a" stop; done

for a in libvirt-bin nova-network nova-compute nova-api nova-objectstore nova-scheduler nova-volume nova-vncproxy; do service "$a" start; done

nova-manage db sync
# create all tables

nova-manage network create private --fixed_range_v4=192.168.1.192/27 --num_networks=1 --bridge=br100 --bridge_interface=eth1 --network_size=32 
# v2
nova-manage network create private --fixed_range_v4=192.168.2.192/27 --num_networks=1 --bridge=br100 --bridge_interface=eth1 --network_size=32 

chown -R nova:nova /etc/nova

for a in libvirt-bin nova-network nova-compute nova-api nova-objectstore nova-scheduler nova-volume nova-vncproxy; do service "$a" stop; done

for a in libvirt-bin nova-network nova-compute nova-api nova-objectstore nova-scheduler nova-volume nova-vncproxy; do service "$a" start; done

nova image-list

ssh-keygen
#Generating public/private rsa key pair.
#Enter file in which to save the key (/root/.ssh/id_rsa): 
#Enter passphrase (empty for no passphrase): 
#Enter same passphrase again: 
#Your identification has been saved in /root/.ssh/id_rsa.
#Your public key has been saved in /root/.ssh/id_rsa.pub.
#The key fingerprint is:
#84:e0:6f:12:6b:51:2d:81:d0:15:59:2b:c2:cb:7f:0d root@ubuntu

nova keypair-add --pub_key .ssh/id_rsa.pub key1
# nova keypair-delete keypairName
nova keypair-delete key1
nova keypair-list

nova image-list
#tty    3a4ae324-50a4-4fed-9ce5-7dd1fcb9f354 
#ubuntu efdec9aa-704c-45be-8d16-5c2bf20a4c63
nova flavor-list
# 1 2 3 4 5

# create a vm
nova boot --flavor 2 --image efdec9aa-704c-45be-8d16-5c2bf20a4c63 --key_name key1 vm1
# id=ccdace9b-d38d-4e29-986d-4ab0f987870e 

# view db
mysql -u root
use nova;
select id,address,instance_id from fixed_ips;
select id,address,fixed_ip_id from floating_ips;

select id from instances;
select id,instance_id,network_info from instance_info_caches;
describe security_group_instance_association;
select id,security_group_id,instance_id from security_group_instance_association;
select id,name from security_groups;

# how to delete instances
SET FOREIGN_KEY_CHECKS=0;
delete from instances where id = '29';
delete from instances where id = '30';
SET FOREIGN_KEY_CHECKS=1;



# nova pause | unpause | resume | reboot | 
nova list
+--------------------------------------+------+--------+-----------------------+
|                  ID                  | Name | Status |        Networks       |
+--------------------------------------+------+--------+-----------------------+
| 999292bc-a25e-4493-b9ad-0ca864049afe | vm1  | ACTIVE | private=192.168.2.194 |
+--------------------------------------+------+--------+-----------------------+

nova show vm1
# id=999292bc-a25e-4493-b9ad-0ca864049afe 
# status=BUILD---->ACTIVE
# private network =192.168.2.194 

#nova delete vmID
nova delete 14c9285d-7709-4594-9f53-df0eb7c5a274  

# status ACTIVE private network 192.168.2.194
ssh -i .ssh/id_rsa ubuntu@192.168.2.194
# Congratuations!
# REMEBER TO exit!
exit
ssh -i .ssh/id_rsa ubuntu@192.168.2.194
ssh ubuntu@192.168.2.194

# scp file
scp tw ubuntu@192.168.2.194:/home/ubuntu/

# Visit by floating IP
nova-manage floating create --ip_range=192.168.1.192/27
nova floating-ip-create
# nova floating-ip-delete 192.168.1.194
+---------------+-------------+----------+------+
|       Ip      | Instance Id | Fixed Ip | Pool |
+---------------+-------------+----------+------+
| 192.168.1.193 | None        | None     | nova |
+---------------+-------------+----------+------+

nova add-floating-ip vm1 192.168.1.193
remove-floating-ip vm1 192.168.1.193
# nova help subcommand
nova list
+--------------------------------------+------+--------+--------------------------------------+
|                  ID                  | Name | Status |               Networks               |
+--------------------------------------+------+--------+--------------------------------------+
| 999292bc-a25e-4493-b9ad-0ca864049afe | vm1  | ACTIVE | private=192.168.2.194, 192.168.1.193 |
+--------------------------------------+------+--------+--------------------------------------+


nova show vm1
# id=20a1f4ce-e48d-4709-8683-efce378b269a
# status=BUILD---->ACTIVE
# private network = 192.168.2.194, 192.168.1.193

# security group rule so that we can visit the VM_ubuntu by floating IP.
nova secgroup-add-rule default tcp 22 22 0.0.0.0/0
+-------------+-----------+---------+-----------+--------------+
| IP Protocol | From Port | To Port |  IP Range | Source Group |
+-------------+-----------+---------+-----------+--------------+
| tcp         | 22        | 22      | 0.0.0.0/0 |              |
+-------------+-----------+---------+-----------+--------------+

nova secgroup-add-rule default icmp -1 -1 0.0.0.0/0
+-------------+-----------+---------+-----------+--------------+
| IP Protocol | From Port | To Port |  IP Range | Source Group |
+-------------+-----------+---------+-----------+--------------+
| icmp        | -1        | -1      | 0.0.0.0/0 |              |
+-------------+-----------+---------+-----------+--------------+

nova secgroup-list
+---------+-------------+
|   Name  | Description |
+---------+-------------+
| default | default     |
+---------+-------------+

nova secgroup-list-rules default
+-------------+-----------+---------+-----------+--------------+
| IP Protocol | From Port | To Port |  IP Range | Source Group |
+-------------+-----------+---------+-----------+--------------+
| icmp        | -1        | -1      | 0.0.0.0/0 |              |
| tcp         | 22        | 22      | 0.0.0.0/0 |              |
+-------------+-----------+---------+-----------+--------------+

nova show vm1
ssh -i .ssh/id_rsa ubuntu@192.168.1.193
# Congratuations!
# REMEBER TO exit!
exit
ssh -i .ssh/id_rsa ubuntu@192.168.1.193
ssh ubuntu@192.168.1.193

# 192.168.1.160----192.168.1.190
# TWO METHODS:
# 1) 160: id_rsa_190--->190:vm1(id_rsa.pub_190, 192.168.2.194, 192.168.1.193)
# REQUIRE: copy id_rsa_190 from 190 to 160
scp  root@192.168.1.190:/root/.ssh/id_rsa_190 chenbin@192.168.1.160:/home/chenbin
ssh -i id_rsa_190 ubuntu@192.168.1.193

# 2) 160: id_rsa_160--->190:vm2(id_rsa.pub_160, 192.168.2.195, 192.168.1.194)
# REQUIRE: copy id_rsa.pub_160 from 160 to 190 
# REUQIRE: then boot an instance using id_rsa.pub_160
scp  root@192.168.1.160:/home/chenbin/.ssh/id_rsa.pub_160 root@192.168.1.190:/root/.ssh/
ssh -i id_rsa_160 ubuntu@192.168.1.194

#CONCLUSION: The second method is recommended!


# logout and login the openstack server 192.168.1.190 ,the first thing you must do is to source novarc and eucarc.
source novarc
source eucarc 
# solution
cat novarc >> .bashrc
cat eucarc >> .bashrc

# Dashboard
apt-get -y install libapache2-mod-wsgi openstack-dashboard
vim /etc/openstack-dashboard/local_settings.py 
#CACHE_BACKEND = 'locmem://'
CACHE_BACKEND = 'memcached://127.0.0.1:11211/'

service apache2 restart

# ssh  22--->7722
# http 80--->7780
# ssh -p 7722 root@192.168.17.91
# http 192.168.17.91:7780

http://192.168.1.190
http://127.0.0.1
http://162.105.17.91:7780
user:admin
pass:kezunlin

# Appendix: ecua-* tools
apt-get install -y euca2ools

export EC2_URL=$(keystone catalog --service ec2 | awk '/ publicURL / { print $4 }')
export CREDS=$(keystone ec2-credentials-create)
export EC2_ACCESS_KEY=$(echo "$CREDS" | awk '/ access / { print $4 }')
export EC2_SECRET_KEY=$(echo "$CREDS" | awk '/ secret / { print $4 }')

env | grep EC2_
EC2_SECRET_KEY=ca9181551a6d454d87b4b0cfcc107a1d
EC2_URL=http://localhost:8773/services/Cloud
EC2_ACCESS_KEY=99d41f7a68ad425a8e7a01ebf6eb4562

# start an instance by using ecua commands
euca-describe-images
nova image-list

euca-describe-instances
nova list 

euca-describe-availability-zones verboese

euca-add-keypair mykey >mykey.priv
# euca-run-instances -k mykey -t m1.tiny $AMI_ID
# $AMI_ID comes from euca-describe-images
euca-describe-images 
# aki-00000001 ari-00000002 ami-00000003
euca-run-instances -k mykey -t m1.tiny ami-00000003
# instance  $VALUE=i-00000001   $IP_ADDR=10.0.0.3
euca-describe-instances
virsh list
Id Name                 State
----------------------------------
  2 instance-00000001    running

# ssh -i mykey.priv $USER@$IP_ADDR
# $USER=ubuntu
# $IP_ADDR(euca-describe-instances)
ssh -i mykey.priv ubuntu@10.0.0.3
# how to terminate the instance when you are done with it
# euca-terminate-instances $VALUE
euca-terminate-instances i-00000001

euca-allocate-address
euca-release-address
euca-release-address 192.168.1.194
euca-associate-address -i i-0000003 192.168.1.194
euca-disassociate-address  192.168.1.194

euca-describe-groups



# Appendix: nova-volume

# create a new file to act as LVM.
dd if=/dev/zero of=/opt/nova-volumes.img bs=1M seek=100000 count=0
#加载一个设备
losetup -f nova-volumes.img
#查看加载情况
losetup -a
/dev/loop0: [0801]:35127298 (/opt/nova-volumes.img)
# 创建一个nova-volume的卷
vgcreate nova-volumes /dev/loop0
#  No physical volume label read from /dev/loop0
#  Physical volume "/dev/loop0" successfully created
#  Volume group "nova-volumes" successfully created

#查看卷
vgdisplay

# create a 1G volume named volume1
nova volume-create --display_name "volume1" 1
nova volume-list
+----+----------+--------------+------+-------------+-------------+
| ID |  Status  | Display Name | Size | Volume Type | Attached to |
+----+----------+--------------+------+-------------+-------------+
| 1  | creating | volume1      | 1    | None        |             |
+----+----------+--------------+------+-------------+-------------+
nova volume-attach VM_ubuntu 1 /dev/vdb

# how to delete volume
euca-describe-volumes
# vol-00000001
euca-delete-volume vol-00000001

